{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graphite.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "32T6UpTmJWeW",
        "outputId": "bee866dc-0d19-4e3c-8dff-ae719f33c1f8"
      },
      "source": [
        "#Using python 2.7, install tensorflow/networkx\r\n",
        "!pip install tensorflow==1.9\r\n",
        "!pip install networkx==1.11"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ff/97d4542f805ae25bf4b65b6263515584c78bd9a6111ed78ea971eff2946a/tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl (51.2MB)\n",
            "\u001b[K     |████████████████████████████████| 51.2MB 67kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (2.0.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (3.8.0)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.1.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.0.post1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (1.16.4)\n",
            "Collecting tensorboard<1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/98/e2e9d5afbc86cef0b2dd0f4ab791519b9bd305ea207e1e5c2f9a9f2f6da6/tensorboard-1.9.0-py2-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.8MB/s \n",
            "\u001b[?25hCollecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.9) (0.8.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.9) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.9) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.9) (5.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow==1.9) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow==1.9) (3.1.1)\n",
            "\u001b[31mERROR: google-auth 1.27.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, setuptools, tensorflow\n",
            "  Found existing installation: tensorboard 2.1.0\n",
            "    Uninstalling tensorboard-2.1.0:\n",
            "      Successfully uninstalled tensorboard-2.1.0\n",
            "  Found existing installation: setuptools 44.1.1\n",
            "    Uninstalling setuptools-44.1.1:\n",
            "      Successfully uninstalled setuptools-44.1.1\n",
            "  Found existing installation: tensorflow 1.3.0\n",
            "    Uninstalling tensorflow-1.3.0:\n",
            "      Successfully uninstalled tensorflow-1.3.0\n",
            "Successfully installed setuptools-39.1.0 tensorboard-1.9.0 tensorflow-1.9.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: networkx==1.11 in /usr/local/lib/python2.7/dist-packages (1.11)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from networkx==1.11) (4.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHWY_LijN5Uh"
      },
      "source": [
        "## Testing Code\r\n",
        "Here we can test code by running the bash commands, like:\r\n",
        "python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.5\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8cxY2s3JtAv",
        "outputId": "90c8000d-e9a2-4277-b064-273592629a83"
      },
      "source": [
        "\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.5\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.81494 train_acc= 0.69646 val_roc= 0.64483 val_ap= 0.66303 test_roc= 0.67086 test_ap= 0.69082\n",
            "Epoch: 0011 train_loss= 0.77914 train_acc= 0.73076 val_roc= 0.69069 val_ap= 0.72648 test_roc= 0.74284 test_ap= 0.77705\n",
            "Epoch: 0021 train_loss= 0.72185 train_acc= 0.73859 val_roc= 0.69867 val_ap= 0.73546 test_roc= 0.74208 test_ap= 0.77315\n",
            "Epoch: 0031 train_loss= 0.66953 train_acc= 0.79938 val_roc= 0.68753 val_ap= 0.71592 test_roc= 0.71839 test_ap= 0.74579\n",
            "Epoch: 0041 train_loss= 0.63636 train_acc= 0.72352 val_roc= 0.66645 val_ap= 0.70332 test_roc= 0.67162 test_ap= 0.71338\n",
            "Epoch: 0051 train_loss= 0.60754 train_acc= 0.74111 val_roc= 0.72275 val_ap= 0.74459 test_roc= 0.72817 test_ap= 0.76291\n",
            "Epoch: 0061 train_loss= 0.57443 train_acc= 0.71415 val_roc= 0.79153 val_ap= 0.80903 test_roc= 0.76920 test_ap= 0.80380\n",
            "Epoch: 0071 train_loss= 0.53979 train_acc= 0.67373 val_roc= 0.82536 val_ap= 0.84447 test_roc= 0.80133 test_ap= 0.83251\n",
            "Epoch: 0081 train_loss= 0.50924 train_acc= 0.68739 val_roc= 0.86031 val_ap= 0.88071 test_roc= 0.83885 test_ap= 0.86792\n",
            "Epoch: 0091 train_loss= 0.49230 train_acc= 0.68211 val_roc= 0.87690 val_ap= 0.89068 test_roc= 0.85615 test_ap= 0.88057\n",
            "Epoch: 0101 train_loss= 0.48083 train_acc= 0.69015 val_roc= 0.88618 val_ap= 0.89398 test_roc= 0.87290 test_ap= 0.89823\n",
            "Epoch: 0111 train_loss= 0.47378 train_acc= 0.69397 val_roc= 0.89411 val_ap= 0.90111 test_roc= 0.87982 test_ap= 0.90558\n",
            "Epoch: 0121 train_loss= 0.46811 train_acc= 0.69855 val_roc= 0.89861 val_ap= 0.90283 test_roc= 0.88680 test_ap= 0.91099\n",
            "Epoch: 0131 train_loss= 0.46242 train_acc= 0.70456 val_roc= 0.90331 val_ap= 0.90766 test_roc= 0.89282 test_ap= 0.91534\n",
            "Epoch: 0141 train_loss= 0.45740 train_acc= 0.70882 val_roc= 0.90901 val_ap= 0.91346 test_roc= 0.89764 test_ap= 0.91915\n",
            "Epoch: 0151 train_loss= 0.45464 train_acc= 0.71353 val_roc= 0.91219 val_ap= 0.91534 test_roc= 0.90015 test_ap= 0.92243\n",
            "Epoch: 0161 train_loss= 0.45182 train_acc= 0.71800 val_roc= 0.91498 val_ap= 0.91896 test_roc= 0.90134 test_ap= 0.92430\n",
            "Epoch: 0171 train_loss= 0.44915 train_acc= 0.72095 val_roc= 0.91917 val_ap= 0.92410 test_roc= 0.90274 test_ap= 0.92536\n",
            "Epoch: 0181 train_loss= 0.44744 train_acc= 0.72525 val_roc= 0.92138 val_ap= 0.92617 test_roc= 0.90345 test_ap= 0.92594\n",
            "Epoch: 0191 train_loss= 0.44560 train_acc= 0.72727 val_roc= 0.92319 val_ap= 0.92694 test_roc= 0.90533 test_ap= 0.92736\n",
            "Epoch: 0201 train_loss= 0.44402 train_acc= 0.72895 val_roc= 0.92731 val_ap= 0.93149 test_roc= 0.90799 test_ap= 0.92897\n",
            "Epoch: 0211 train_loss= 0.44275 train_acc= 0.73254 val_roc= 0.93101 val_ap= 0.93475 test_roc= 0.91065 test_ap= 0.93082\n",
            "Epoch: 0221 train_loss= 0.44072 train_acc= 0.73534 val_roc= 0.93329 val_ap= 0.93740 test_roc= 0.91217 test_ap= 0.93184\n",
            "Epoch: 0231 train_loss= 0.43915 train_acc= 0.73854 val_roc= 0.93539 val_ap= 0.94035 test_roc= 0.91396 test_ap= 0.93285\n",
            "Epoch: 0241 train_loss= 0.43830 train_acc= 0.74136 val_roc= 0.93689 val_ap= 0.94195 test_roc= 0.91580 test_ap= 0.93352\n",
            "Epoch: 0251 train_loss= 0.43761 train_acc= 0.74271 val_roc= 0.93788 val_ap= 0.94285 test_roc= 0.91724 test_ap= 0.93447\n",
            "Epoch: 0261 train_loss= 0.43665 train_acc= 0.74372 val_roc= 0.93952 val_ap= 0.94416 test_roc= 0.91828 test_ap= 0.93522\n",
            "Epoch: 0271 train_loss= 0.43612 train_acc= 0.74711 val_roc= 0.94168 val_ap= 0.94609 test_roc= 0.91965 test_ap= 0.93604\n",
            "Epoch: 0281 train_loss= 0.43510 train_acc= 0.74760 val_roc= 0.94323 val_ap= 0.94754 test_roc= 0.92049 test_ap= 0.93683\n",
            "Epoch: 0291 train_loss= 0.43413 train_acc= 0.75144 val_roc= 0.94518 val_ap= 0.94984 test_roc= 0.92166 test_ap= 0.93764\n",
            "Epoch: 0301 train_loss= 0.43233 train_acc= 0.75394 val_roc= 0.94618 val_ap= 0.95105 test_roc= 0.92191 test_ap= 0.93761\n",
            "Epoch: 0311 train_loss= 0.43142 train_acc= 0.75557 val_roc= 0.94727 val_ap= 0.95225 test_roc= 0.92205 test_ap= 0.93779\n",
            "Epoch: 0321 train_loss= 0.42981 train_acc= 0.75670 val_roc= 0.94762 val_ap= 0.95295 test_roc= 0.92065 test_ap= 0.93709\n",
            "Epoch: 0331 train_loss= 0.42863 train_acc= 0.75934 val_roc= 0.94701 val_ap= 0.95244 test_roc= 0.91918 test_ap= 0.93627\n",
            "Epoch: 0341 train_loss= 0.42818 train_acc= 0.76132 val_roc= 0.94667 val_ap= 0.95241 test_roc= 0.91791 test_ap= 0.93524\n",
            "Epoch: 0351 train_loss= 0.42712 train_acc= 0.76188 val_roc= 0.94602 val_ap= 0.95245 test_roc= 0.91645 test_ap= 0.93446\n",
            "Epoch: 0361 train_loss= 0.42686 train_acc= 0.76481 val_roc= 0.94594 val_ap= 0.95199 test_roc= 0.91662 test_ap= 0.93472\n",
            "Epoch: 0371 train_loss= 0.42614 train_acc= 0.76584 val_roc= 0.94558 val_ap= 0.95110 test_roc= 0.91606 test_ap= 0.93410\n",
            "Epoch: 0381 train_loss= 0.42521 train_acc= 0.76721 val_roc= 0.94667 val_ap= 0.95188 test_roc= 0.91627 test_ap= 0.93403\n",
            "Epoch: 0391 train_loss= 0.42448 train_acc= 0.76876 val_roc= 0.94703 val_ap= 0.95176 test_roc= 0.91551 test_ap= 0.93358\n",
            "Epoch: 0401 train_loss= 0.42414 train_acc= 0.77058 val_roc= 0.94723 val_ap= 0.95120 test_roc= 0.91538 test_ap= 0.93337\n",
            "Epoch: 0411 train_loss= 0.42376 train_acc= 0.77059 val_roc= 0.94712 val_ap= 0.95135 test_roc= 0.91525 test_ap= 0.93293\n",
            "Epoch: 0421 train_loss= 0.42340 train_acc= 0.77314 val_roc= 0.94735 val_ap= 0.95218 test_roc= 0.91451 test_ap= 0.93246\n",
            "Epoch: 0431 train_loss= 0.42282 train_acc= 0.77406 val_roc= 0.94707 val_ap= 0.95165 test_roc= 0.91408 test_ap= 0.93200\n",
            "Epoch: 0441 train_loss= 0.42231 train_acc= 0.77682 val_roc= 0.94563 val_ap= 0.94959 test_roc= 0.91446 test_ap= 0.93231\n",
            "Epoch: 0451 train_loss= 0.42152 train_acc= 0.77729 val_roc= 0.94574 val_ap= 0.94952 test_roc= 0.91555 test_ap= 0.93318\n",
            "Epoch: 0461 train_loss= 0.42135 train_acc= 0.77877 val_roc= 0.94560 val_ap= 0.94937 test_roc= 0.91604 test_ap= 0.93377\n",
            "Epoch: 0471 train_loss= 0.42108 train_acc= 0.77988 val_roc= 0.94473 val_ap= 0.94873 test_roc= 0.91588 test_ap= 0.93365\n",
            "Epoch: 0481 train_loss= 0.42101 train_acc= 0.77974 val_roc= 0.94343 val_ap= 0.94733 test_roc= 0.91605 test_ap= 0.93375\n",
            "Epoch: 0491 train_loss= 0.42046 train_acc= 0.78062 val_roc= 0.94205 val_ap= 0.94611 test_roc= 0.91642 test_ap= 0.93406\n",
            "Best epoch (from validation):  321\n",
            "test_roc  0.9206528666433826\n",
            "test_ap  0.9371647169841318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brndpxlHO8__",
        "outputId": "f52ab4a6-fa7b-446c-c207-11263e2225d3"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap, lambda=0\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 1.70175 train_acc= 0.55239 val_roc= 0.70905 val_ap= 0.70852 test_roc= 0.70745 test_ap= 0.71996\n",
            "Epoch: 0011 train_loss= 1.30582 train_acc= 0.56647 val_roc= 0.72009 val_ap= 0.74398 test_roc= 0.75390 test_ap= 0.77427\n",
            "Epoch: 0021 train_loss= 0.79658 train_acc= 0.60153 val_roc= 0.71681 val_ap= 0.74445 test_roc= 0.74812 test_ap= 0.77533\n",
            "Epoch: 0031 train_loss= 0.70021 train_acc= 0.68991 val_roc= 0.71661 val_ap= 0.74097 test_roc= 0.74154 test_ap= 0.77232\n",
            "Epoch: 0041 train_loss= 0.65052 train_acc= 0.77198 val_roc= 0.71753 val_ap= 0.73497 test_roc= 0.72746 test_ap= 0.75784\n",
            "Epoch: 0051 train_loss= 0.60729 train_acc= 0.69033 val_roc= 0.72787 val_ap= 0.73857 test_roc= 0.72597 test_ap= 0.75450\n",
            "Epoch: 0061 train_loss= 0.53662 train_acc= 0.68595 val_roc= 0.81505 val_ap= 0.83722 test_roc= 0.80901 test_ap= 0.83941\n",
            "Epoch: 0071 train_loss= 0.50974 train_acc= 0.65647 val_roc= 0.85011 val_ap= 0.86694 test_roc= 0.83843 test_ap= 0.86438\n",
            "Epoch: 0081 train_loss= 0.48644 train_acc= 0.67369 val_roc= 0.87415 val_ap= 0.89684 test_roc= 0.85804 test_ap= 0.88329\n",
            "Epoch: 0091 train_loss= 0.47341 train_acc= 0.69025 val_roc= 0.88790 val_ap= 0.91208 test_roc= 0.87312 test_ap= 0.89897\n",
            "Epoch: 0101 train_loss= 0.46546 train_acc= 0.69024 val_roc= 0.89346 val_ap= 0.91680 test_roc= 0.87997 test_ap= 0.90573\n",
            "Epoch: 0111 train_loss= 0.45901 train_acc= 0.69996 val_roc= 0.89838 val_ap= 0.92152 test_roc= 0.88427 test_ap= 0.91077\n",
            "Epoch: 0121 train_loss= 0.45322 train_acc= 0.70396 val_roc= 0.90178 val_ap= 0.92482 test_roc= 0.88835 test_ap= 0.91539\n",
            "Epoch: 0131 train_loss= 0.44897 train_acc= 0.70937 val_roc= 0.90282 val_ap= 0.92519 test_roc= 0.89153 test_ap= 0.91852\n",
            "Epoch: 0141 train_loss= 0.44638 train_acc= 0.71374 val_roc= 0.90500 val_ap= 0.92527 test_roc= 0.89501 test_ap= 0.92127\n",
            "Epoch: 0151 train_loss= 0.44459 train_acc= 0.71823 val_roc= 0.90733 val_ap= 0.92762 test_roc= 0.89722 test_ap= 0.92323\n",
            "Epoch: 0161 train_loss= 0.44269 train_acc= 0.72124 val_roc= 0.90857 val_ap= 0.92786 test_roc= 0.89861 test_ap= 0.92423\n",
            "Epoch: 0171 train_loss= 0.44111 train_acc= 0.72402 val_roc= 0.91159 val_ap= 0.93058 test_roc= 0.90034 test_ap= 0.92519\n",
            "Epoch: 0181 train_loss= 0.43944 train_acc= 0.72897 val_roc= 0.91318 val_ap= 0.93023 test_roc= 0.90153 test_ap= 0.92572\n",
            "Epoch: 0191 train_loss= 0.43775 train_acc= 0.73322 val_roc= 0.91512 val_ap= 0.93202 test_roc= 0.90311 test_ap= 0.92662\n",
            "Epoch: 0201 train_loss= 0.43607 train_acc= 0.73446 val_roc= 0.91755 val_ap= 0.93371 test_roc= 0.90472 test_ap= 0.92805\n",
            "Epoch: 0211 train_loss= 0.43520 train_acc= 0.73889 val_roc= 0.91782 val_ap= 0.93454 test_roc= 0.90473 test_ap= 0.92835\n",
            "Epoch: 0221 train_loss= 0.43420 train_acc= 0.74034 val_roc= 0.92090 val_ap= 0.93624 test_roc= 0.90543 test_ap= 0.92916\n",
            "Epoch: 0231 train_loss= 0.43271 train_acc= 0.74309 val_roc= 0.92140 val_ap= 0.93671 test_roc= 0.90548 test_ap= 0.92907\n",
            "Epoch: 0241 train_loss= 0.43218 train_acc= 0.74749 val_roc= 0.92359 val_ap= 0.93792 test_roc= 0.90677 test_ap= 0.93009\n",
            "Epoch: 0251 train_loss= 0.43117 train_acc= 0.74963 val_roc= 0.92445 val_ap= 0.93801 test_roc= 0.90881 test_ap= 0.93157\n",
            "Epoch: 0261 train_loss= 0.43011 train_acc= 0.74970 val_roc= 0.92554 val_ap= 0.93867 test_roc= 0.90919 test_ap= 0.93155\n",
            "Epoch: 0271 train_loss= 0.42948 train_acc= 0.75236 val_roc= 0.92800 val_ap= 0.94003 test_roc= 0.90998 test_ap= 0.93245\n",
            "Epoch: 0281 train_loss= 0.42852 train_acc= 0.75403 val_roc= 0.92805 val_ap= 0.94009 test_roc= 0.91025 test_ap= 0.93251\n",
            "Epoch: 0291 train_loss= 0.42796 train_acc= 0.75555 val_roc= 0.92909 val_ap= 0.94095 test_roc= 0.91093 test_ap= 0.93284\n",
            "Epoch: 0301 train_loss= 0.42713 train_acc= 0.75707 val_roc= 0.92865 val_ap= 0.94079 test_roc= 0.91119 test_ap= 0.93296\n",
            "Epoch: 0311 train_loss= 0.42640 train_acc= 0.75960 val_roc= 0.92835 val_ap= 0.94106 test_roc= 0.91237 test_ap= 0.93374\n",
            "Epoch: 0321 train_loss= 0.42578 train_acc= 0.76078 val_roc= 0.92854 val_ap= 0.94155 test_roc= 0.91308 test_ap= 0.93418\n",
            "Epoch: 0331 train_loss= 0.42519 train_acc= 0.76248 val_roc= 0.92867 val_ap= 0.94153 test_roc= 0.91362 test_ap= 0.93471\n",
            "Epoch: 0341 train_loss= 0.42493 train_acc= 0.76462 val_roc= 0.92870 val_ap= 0.94150 test_roc= 0.91414 test_ap= 0.93490\n",
            "Epoch: 0351 train_loss= 0.42445 train_acc= 0.76570 val_roc= 0.92779 val_ap= 0.94140 test_roc= 0.91436 test_ap= 0.93507\n",
            "Epoch: 0361 train_loss= 0.42382 train_acc= 0.76636 val_roc= 0.92806 val_ap= 0.94103 test_roc= 0.91538 test_ap= 0.93600\n",
            "Epoch: 0371 train_loss= 0.42356 train_acc= 0.76739 val_roc= 0.92812 val_ap= 0.94080 test_roc= 0.91524 test_ap= 0.93586\n",
            "Epoch: 0381 train_loss= 0.42347 train_acc= 0.76771 val_roc= 0.92893 val_ap= 0.94148 test_roc= 0.91602 test_ap= 0.93642\n",
            "Epoch: 0391 train_loss= 0.42317 train_acc= 0.76981 val_roc= 0.92839 val_ap= 0.94063 test_roc= 0.91609 test_ap= 0.93614\n",
            "Epoch: 0401 train_loss= 0.42335 train_acc= 0.77060 val_roc= 0.92767 val_ap= 0.94011 test_roc= 0.91612 test_ap= 0.93641\n",
            "Epoch: 0411 train_loss= 0.42320 train_acc= 0.77261 val_roc= 0.92751 val_ap= 0.93935 test_roc= 0.91698 test_ap= 0.93679\n",
            "Epoch: 0421 train_loss= 0.42320 train_acc= 0.77306 val_roc= 0.92719 val_ap= 0.93916 test_roc= 0.91677 test_ap= 0.93669\n",
            "Epoch: 0431 train_loss= 0.42298 train_acc= 0.77305 val_roc= 0.92741 val_ap= 0.93932 test_roc= 0.91672 test_ap= 0.93642\n",
            "Epoch: 0441 train_loss= 0.42268 train_acc= 0.77465 val_roc= 0.92774 val_ap= 0.93989 test_roc= 0.91742 test_ap= 0.93686\n",
            "Epoch: 0451 train_loss= 0.42254 train_acc= 0.77460 val_roc= 0.92745 val_ap= 0.93947 test_roc= 0.91781 test_ap= 0.93751\n",
            "Epoch: 0461 train_loss= 0.42254 train_acc= 0.77589 val_roc= 0.92774 val_ap= 0.93921 test_roc= 0.91785 test_ap= 0.93744\n",
            "Epoch: 0471 train_loss= 0.42254 train_acc= 0.77876 val_roc= 0.92789 val_ap= 0.93957 test_roc= 0.91791 test_ap= 0.93735\n",
            "Epoch: 0481 train_loss= 0.42257 train_acc= 0.77785 val_roc= 0.92802 val_ap= 0.93944 test_roc= 0.91847 test_ap= 0.93760\n",
            "Epoch: 0491 train_loss= 0.42207 train_acc= 0.77901 val_roc= 0.92815 val_ap= 0.93981 test_roc= 0.91860 test_ap= 0.93760\n",
            "Best epoch (from validation):  313\n",
            "test_roc  0.9122777959809742\n",
            "test_ap  0.9336263831928271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvXX94poQXMn",
        "outputId": "fe058cd8-d1b2-4397-ebda-ddf97eb4dda3"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap, lambda=0.1\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.1\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 1.42890 train_acc= 0.56506 val_roc= 0.68839 val_ap= 0.65700 test_roc= 0.70719 test_ap= 0.71382\n",
            "Epoch: 0011 train_loss= 1.18472 train_acc= 0.58143 val_roc= 0.74912 val_ap= 0.75478 test_roc= 0.75193 test_ap= 0.77755\n",
            "Epoch: 0021 train_loss= 0.81230 train_acc= 0.63654 val_roc= 0.73078 val_ap= 0.74344 test_roc= 0.75556 test_ap= 0.77611\n",
            "Epoch: 0031 train_loss= 0.70377 train_acc= 0.72533 val_roc= 0.72720 val_ap= 0.73515 test_roc= 0.74787 test_ap= 0.76641\n",
            "Epoch: 0041 train_loss= 0.64148 train_acc= 0.79398 val_roc= 0.68703 val_ap= 0.71749 test_roc= 0.69166 test_ap= 0.73114\n",
            "Epoch: 0051 train_loss= 0.59609 train_acc= 0.70733 val_roc= 0.72373 val_ap= 0.74447 test_roc= 0.71542 test_ap= 0.74853\n",
            "Epoch: 0061 train_loss= 0.54037 train_acc= 0.68311 val_roc= 0.81234 val_ap= 0.82910 test_roc= 0.79816 test_ap= 0.80850\n",
            "Epoch: 0071 train_loss= 0.51103 train_acc= 0.66812 val_roc= 0.83783 val_ap= 0.85726 test_roc= 0.83372 test_ap= 0.85558\n",
            "Epoch: 0081 train_loss= 0.49291 train_acc= 0.68026 val_roc= 0.85301 val_ap= 0.87221 test_roc= 0.84314 test_ap= 0.87376\n",
            "Epoch: 0091 train_loss= 0.48121 train_acc= 0.68719 val_roc= 0.87487 val_ap= 0.89101 test_roc= 0.84771 test_ap= 0.87962\n",
            "Epoch: 0101 train_loss= 0.47284 train_acc= 0.69239 val_roc= 0.88855 val_ap= 0.90095 test_roc= 0.85773 test_ap= 0.88965\n",
            "Epoch: 0111 train_loss= 0.46643 train_acc= 0.70046 val_roc= 0.89809 val_ap= 0.91082 test_roc= 0.86618 test_ap= 0.89780\n",
            "Epoch: 0121 train_loss= 0.46019 train_acc= 0.70486 val_roc= 0.90416 val_ap= 0.91737 test_roc= 0.87419 test_ap= 0.90441\n",
            "Epoch: 0131 train_loss= 0.45549 train_acc= 0.70800 val_roc= 0.90649 val_ap= 0.92208 test_roc= 0.88043 test_ap= 0.90914\n",
            "Epoch: 0141 train_loss= 0.45080 train_acc= 0.71372 val_roc= 0.90850 val_ap= 0.92501 test_roc= 0.88527 test_ap= 0.91307\n",
            "Epoch: 0151 train_loss= 0.44658 train_acc= 0.71899 val_roc= 0.91177 val_ap= 0.92878 test_roc= 0.89075 test_ap= 0.91649\n",
            "Epoch: 0161 train_loss= 0.44244 train_acc= 0.72706 val_roc= 0.91428 val_ap= 0.93103 test_roc= 0.89643 test_ap= 0.91996\n",
            "Epoch: 0171 train_loss= 0.43926 train_acc= 0.73191 val_roc= 0.91618 val_ap= 0.93224 test_roc= 0.90057 test_ap= 0.92271\n",
            "Epoch: 0181 train_loss= 0.43705 train_acc= 0.73811 val_roc= 0.91840 val_ap= 0.93332 test_roc= 0.90426 test_ap= 0.92563\n",
            "Epoch: 0191 train_loss= 0.43468 train_acc= 0.74186 val_roc= 0.92109 val_ap= 0.93573 test_roc= 0.90698 test_ap= 0.92766\n",
            "Epoch: 0201 train_loss= 0.43305 train_acc= 0.74545 val_roc= 0.92270 val_ap= 0.93800 test_roc= 0.90847 test_ap= 0.92892\n",
            "Epoch: 0211 train_loss= 0.43165 train_acc= 0.74952 val_roc= 0.92384 val_ap= 0.93847 test_roc= 0.90997 test_ap= 0.93051\n",
            "Epoch: 0221 train_loss= 0.43024 train_acc= 0.75190 val_roc= 0.92491 val_ap= 0.93960 test_roc= 0.91105 test_ap= 0.93159\n",
            "Epoch: 0231 train_loss= 0.42889 train_acc= 0.75649 val_roc= 0.92569 val_ap= 0.94094 test_roc= 0.91096 test_ap= 0.93143\n",
            "Epoch: 0241 train_loss= 0.42841 train_acc= 0.75695 val_roc= 0.92654 val_ap= 0.94107 test_roc= 0.91221 test_ap= 0.93249\n",
            "Epoch: 0251 train_loss= 0.42747 train_acc= 0.76082 val_roc= 0.92740 val_ap= 0.94179 test_roc= 0.91283 test_ap= 0.93316\n",
            "Epoch: 0261 train_loss= 0.42618 train_acc= 0.76226 val_roc= 0.92815 val_ap= 0.94200 test_roc= 0.91359 test_ap= 0.93406\n",
            "Epoch: 0271 train_loss= 0.42585 train_acc= 0.76544 val_roc= 0.92904 val_ap= 0.94283 test_roc= 0.91422 test_ap= 0.93447\n",
            "Epoch: 0281 train_loss= 0.42508 train_acc= 0.76600 val_roc= 0.92955 val_ap= 0.94267 test_roc= 0.91445 test_ap= 0.93481\n",
            "Epoch: 0291 train_loss= 0.42449 train_acc= 0.76861 val_roc= 0.92861 val_ap= 0.94269 test_roc= 0.91461 test_ap= 0.93523\n",
            "Epoch: 0301 train_loss= 0.42414 train_acc= 0.76920 val_roc= 0.92835 val_ap= 0.94274 test_roc= 0.91469 test_ap= 0.93507\n",
            "Epoch: 0311 train_loss= 0.42400 train_acc= 0.77066 val_roc= 0.92832 val_ap= 0.94289 test_roc= 0.91432 test_ap= 0.93482\n",
            "Epoch: 0321 train_loss= 0.42361 train_acc= 0.77053 val_roc= 0.92807 val_ap= 0.94206 test_roc= 0.91432 test_ap= 0.93483\n",
            "Epoch: 0331 train_loss= 0.42322 train_acc= 0.77252 val_roc= 0.92747 val_ap= 0.94189 test_roc= 0.91478 test_ap= 0.93538\n",
            "Epoch: 0341 train_loss= 0.42317 train_acc= 0.77269 val_roc= 0.92738 val_ap= 0.94245 test_roc= 0.91527 test_ap= 0.93526\n",
            "Epoch: 0351 train_loss= 0.42278 train_acc= 0.77316 val_roc= 0.92683 val_ap= 0.94199 test_roc= 0.91528 test_ap= 0.93560\n",
            "Epoch: 0361 train_loss= 0.42268 train_acc= 0.77421 val_roc= 0.92569 val_ap= 0.94078 test_roc= 0.91523 test_ap= 0.93563\n",
            "Epoch: 0371 train_loss= 0.42256 train_acc= 0.77589 val_roc= 0.92690 val_ap= 0.94250 test_roc= 0.91591 test_ap= 0.93616\n",
            "Epoch: 0381 train_loss= 0.42227 train_acc= 0.77621 val_roc= 0.92556 val_ap= 0.94130 test_roc= 0.91610 test_ap= 0.93611\n",
            "Epoch: 0391 train_loss= 0.42212 train_acc= 0.77730 val_roc= 0.92423 val_ap= 0.93935 test_roc= 0.91634 test_ap= 0.93615\n",
            "Epoch: 0401 train_loss= 0.42225 train_acc= 0.77876 val_roc= 0.92482 val_ap= 0.94072 test_roc= 0.91679 test_ap= 0.93630\n",
            "Epoch: 0411 train_loss= 0.42237 train_acc= 0.77849 val_roc= 0.92290 val_ap= 0.93925 test_roc= 0.91624 test_ap= 0.93570\n",
            "Epoch: 0421 train_loss= 0.42223 train_acc= 0.77985 val_roc= 0.92271 val_ap= 0.94004 test_roc= 0.91650 test_ap= 0.93600\n",
            "Epoch: 0431 train_loss= 0.42213 train_acc= 0.78122 val_roc= 0.92161 val_ap= 0.93819 test_roc= 0.91650 test_ap= 0.93594\n",
            "Epoch: 0441 train_loss= 0.42207 train_acc= 0.78327 val_roc= 0.92164 val_ap= 0.93853 test_roc= 0.91640 test_ap= 0.93564\n",
            "Epoch: 0451 train_loss= 0.42176 train_acc= 0.78270 val_roc= 0.92142 val_ap= 0.93824 test_roc= 0.91737 test_ap= 0.93657\n",
            "Epoch: 0461 train_loss= 0.42175 train_acc= 0.78338 val_roc= 0.92099 val_ap= 0.93879 test_roc= 0.91714 test_ap= 0.93649\n",
            "Epoch: 0471 train_loss= 0.42182 train_acc= 0.78447 val_roc= 0.92041 val_ap= 0.93918 test_roc= 0.91697 test_ap= 0.93626\n",
            "Epoch: 0481 train_loss= 0.42162 train_acc= 0.78526 val_roc= 0.92018 val_ap= 0.93837 test_roc= 0.91724 test_ap= 0.93597\n",
            "Epoch: 0491 train_loss= 0.42144 train_acc= 0.78635 val_roc= 0.91895 val_ap= 0.93727 test_roc= 0.91721 test_ap= 0.93589\n",
            "Best epoch (from validation):  286\n",
            "test_roc  0.9146218075894128\n",
            "test_ap  0.9352252249462428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKNkAKz6QXWh",
        "outputId": "b7509185-266b-430f-8f9c-9a7bd283145f"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap, lambda = 0.2\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 1.72197 train_acc= 0.55286 val_roc= 0.67861 val_ap= 0.69482 test_roc= 0.68627 test_ap= 0.69730\n",
            "Epoch: 0011 train_loss= 1.40611 train_acc= 0.56254 val_roc= 0.71160 val_ap= 0.73191 test_roc= 0.76564 test_ap= 0.77860\n",
            "Epoch: 0021 train_loss= 0.88782 train_acc= 0.57697 val_roc= 0.71642 val_ap= 0.73557 test_roc= 0.76437 test_ap= 0.77757\n",
            "Epoch: 0031 train_loss= 0.71443 train_acc= 0.67583 val_roc= 0.71920 val_ap= 0.74025 test_roc= 0.75617 test_ap= 0.77010\n",
            "Epoch: 0041 train_loss= 0.67102 train_acc= 0.75798 val_roc= 0.71798 val_ap= 0.73508 test_roc= 0.73777 test_ap= 0.75366\n",
            "Epoch: 0051 train_loss= 0.61536 train_acc= 0.70884 val_roc= 0.72266 val_ap= 0.74015 test_roc= 0.72636 test_ap= 0.75201\n",
            "Epoch: 0061 train_loss= 0.55995 train_acc= 0.67804 val_roc= 0.78052 val_ap= 0.79915 test_roc= 0.78572 test_ap= 0.81734\n",
            "Epoch: 0071 train_loss= 0.53197 train_acc= 0.66191 val_roc= 0.82161 val_ap= 0.82891 test_roc= 0.81295 test_ap= 0.84186\n",
            "Epoch: 0081 train_loss= 0.50468 train_acc= 0.66494 val_roc= 0.85989 val_ap= 0.86945 test_roc= 0.83463 test_ap= 0.86124\n",
            "Epoch: 0091 train_loss= 0.48780 train_acc= 0.67725 val_roc= 0.88997 val_ap= 0.90133 test_roc= 0.85346 test_ap= 0.87331\n",
            "Epoch: 0101 train_loss= 0.47779 train_acc= 0.68338 val_roc= 0.90066 val_ap= 0.91047 test_roc= 0.86374 test_ap= 0.88251\n",
            "Epoch: 0111 train_loss= 0.46937 train_acc= 0.68938 val_roc= 0.90824 val_ap= 0.91786 test_roc= 0.87256 test_ap= 0.89238\n",
            "Epoch: 0121 train_loss= 0.46324 train_acc= 0.69786 val_roc= 0.91626 val_ap= 0.92601 test_roc= 0.87970 test_ap= 0.90169\n",
            "Epoch: 0131 train_loss= 0.45733 train_acc= 0.70251 val_roc= 0.92057 val_ap= 0.93021 test_roc= 0.88609 test_ap= 0.90936\n",
            "Epoch: 0141 train_loss= 0.45285 train_acc= 0.70958 val_roc= 0.92121 val_ap= 0.93177 test_roc= 0.89209 test_ap= 0.91536\n",
            "Epoch: 0151 train_loss= 0.44775 train_acc= 0.71802 val_roc= 0.92268 val_ap= 0.93445 test_roc= 0.89546 test_ap= 0.91840\n",
            "Epoch: 0161 train_loss= 0.44369 train_acc= 0.72307 val_roc= 0.92047 val_ap= 0.93458 test_roc= 0.89707 test_ap= 0.91962\n",
            "Epoch: 0171 train_loss= 0.44057 train_acc= 0.72903 val_roc= 0.92109 val_ap= 0.93590 test_roc= 0.89788 test_ap= 0.92014\n",
            "Epoch: 0181 train_loss= 0.43833 train_acc= 0.73531 val_roc= 0.91930 val_ap= 0.93536 test_roc= 0.89835 test_ap= 0.91996\n",
            "Epoch: 0191 train_loss= 0.43578 train_acc= 0.73964 val_roc= 0.92025 val_ap= 0.93622 test_roc= 0.90066 test_ap= 0.92162\n",
            "Epoch: 0201 train_loss= 0.43428 train_acc= 0.74347 val_roc= 0.91962 val_ap= 0.93619 test_roc= 0.90121 test_ap= 0.92112\n",
            "Epoch: 0211 train_loss= 0.43318 train_acc= 0.74714 val_roc= 0.91953 val_ap= 0.93551 test_roc= 0.90342 test_ap= 0.92231\n",
            "Epoch: 0221 train_loss= 0.43176 train_acc= 0.75210 val_roc= 0.91849 val_ap= 0.93502 test_roc= 0.90327 test_ap= 0.92190\n",
            "Epoch: 0231 train_loss= 0.43008 train_acc= 0.75640 val_roc= 0.91859 val_ap= 0.93475 test_roc= 0.90520 test_ap= 0.92314\n",
            "Epoch: 0241 train_loss= 0.42913 train_acc= 0.76173 val_roc= 0.91904 val_ap= 0.93533 test_roc= 0.90701 test_ap= 0.92450\n",
            "Epoch: 0251 train_loss= 0.42794 train_acc= 0.76287 val_roc= 0.91745 val_ap= 0.93290 test_roc= 0.90795 test_ap= 0.92529\n",
            "Epoch: 0261 train_loss= 0.42733 train_acc= 0.76462 val_roc= 0.92005 val_ap= 0.93491 test_roc= 0.90927 test_ap= 0.92626\n",
            "Epoch: 0271 train_loss= 0.42692 train_acc= 0.76794 val_roc= 0.92093 val_ap= 0.93525 test_roc= 0.90983 test_ap= 0.92634\n",
            "Epoch: 0281 train_loss= 0.42621 train_acc= 0.77014 val_roc= 0.92163 val_ap= 0.93514 test_roc= 0.91165 test_ap= 0.92770\n",
            "Epoch: 0291 train_loss= 0.42555 train_acc= 0.77232 val_roc= 0.92320 val_ap= 0.93594 test_roc= 0.91315 test_ap= 0.92862\n",
            "Epoch: 0301 train_loss= 0.42495 train_acc= 0.77430 val_roc= 0.92385 val_ap= 0.93639 test_roc= 0.91384 test_ap= 0.92885\n",
            "Epoch: 0311 train_loss= 0.42431 train_acc= 0.77597 val_roc= 0.92498 val_ap= 0.93717 test_roc= 0.91553 test_ap= 0.93008\n",
            "Epoch: 0321 train_loss= 0.42386 train_acc= 0.77697 val_roc= 0.92609 val_ap= 0.93730 test_roc= 0.91668 test_ap= 0.93132\n",
            "Epoch: 0331 train_loss= 0.42384 train_acc= 0.77819 val_roc= 0.92687 val_ap= 0.93775 test_roc= 0.91727 test_ap= 0.93145\n",
            "Epoch: 0341 train_loss= 0.42376 train_acc= 0.77996 val_roc= 0.92648 val_ap= 0.93703 test_roc= 0.91783 test_ap= 0.93190\n",
            "Epoch: 0351 train_loss= 0.42330 train_acc= 0.77981 val_roc= 0.92682 val_ap= 0.93780 test_roc= 0.91863 test_ap= 0.93191\n",
            "Epoch: 0361 train_loss= 0.42311 train_acc= 0.78287 val_roc= 0.92761 val_ap= 0.93808 test_roc= 0.91890 test_ap= 0.93197\n",
            "Epoch: 0371 train_loss= 0.42299 train_acc= 0.78309 val_roc= 0.92722 val_ap= 0.93840 test_roc= 0.91938 test_ap= 0.93240\n",
            "Epoch: 0381 train_loss= 0.42284 train_acc= 0.78400 val_roc= 0.92858 val_ap= 0.93917 test_roc= 0.91975 test_ap= 0.93287\n",
            "Epoch: 0391 train_loss= 0.42266 train_acc= 0.78621 val_roc= 0.92935 val_ap= 0.93908 test_roc= 0.91998 test_ap= 0.93282\n",
            "Epoch: 0401 train_loss= 0.42264 train_acc= 0.78751 val_roc= 0.92883 val_ap= 0.93951 test_roc= 0.92039 test_ap= 0.93307\n",
            "Epoch: 0411 train_loss= 0.42268 train_acc= 0.78770 val_roc= 0.92968 val_ap= 0.93971 test_roc= 0.92013 test_ap= 0.93275\n",
            "Epoch: 0421 train_loss= 0.42258 train_acc= 0.78900 val_roc= 0.93143 val_ap= 0.94021 test_roc= 0.92026 test_ap= 0.93268\n",
            "Epoch: 0431 train_loss= 0.42255 train_acc= 0.79095 val_roc= 0.93157 val_ap= 0.94030 test_roc= 0.92047 test_ap= 0.93308\n",
            "Epoch: 0441 train_loss= 0.42255 train_acc= 0.79268 val_roc= 0.93244 val_ap= 0.94117 test_roc= 0.92108 test_ap= 0.93348\n",
            "Epoch: 0451 train_loss= 0.42220 train_acc= 0.79201 val_roc= 0.93325 val_ap= 0.94198 test_roc= 0.92104 test_ap= 0.93333\n",
            "Epoch: 0461 train_loss= 0.42218 train_acc= 0.79380 val_roc= 0.93319 val_ap= 0.94138 test_roc= 0.92175 test_ap= 0.93365\n",
            "Epoch: 0471 train_loss= 0.42228 train_acc= 0.79445 val_roc= 0.93383 val_ap= 0.94222 test_roc= 0.92110 test_ap= 0.93348\n",
            "Epoch: 0481 train_loss= 0.42204 train_acc= 0.79528 val_roc= 0.93387 val_ap= 0.94214 test_roc= 0.92184 test_ap= 0.93424\n",
            "Epoch: 0491 train_loss= 0.42172 train_acc= 0.79651 val_roc= 0.93431 val_ap= 0.94229 test_roc= 0.92119 test_ap= 0.93343\n",
            "Best epoch (from validation):  483\n",
            "test_roc  0.9218554778219056\n",
            "test_ap  0.9339847746089402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVLxIGHPQfFG",
        "outputId": "226eec2a-ebad-4259-91c1-3bb685ef853e"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap, lambda = 0.3\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.3\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 1.04724 train_acc= 0.60481 val_roc= 0.64034 val_ap= 0.64349 test_roc= 0.68936 test_ap= 0.67930\n",
            "Epoch: 0011 train_loss= 0.92539 train_acc= 0.62628 val_roc= 0.72319 val_ap= 0.75619 test_roc= 0.75032 test_ap= 0.77945\n",
            "Epoch: 0021 train_loss= 0.75196 train_acc= 0.64142 val_roc= 0.72222 val_ap= 0.74972 test_roc= 0.75182 test_ap= 0.77712\n",
            "Epoch: 0031 train_loss= 0.67614 train_acc= 0.74811 val_roc= 0.71010 val_ap= 0.73360 test_roc= 0.73415 test_ap= 0.75550\n",
            "Epoch: 0041 train_loss= 0.63736 train_acc= 0.75469 val_roc= 0.68249 val_ap= 0.71616 test_roc= 0.68248 test_ap= 0.72021\n",
            "Epoch: 0051 train_loss= 0.60506 train_acc= 0.72579 val_roc= 0.73456 val_ap= 0.75277 test_roc= 0.72948 test_ap= 0.75673\n",
            "Epoch: 0061 train_loss= 0.55528 train_acc= 0.70824 val_roc= 0.80260 val_ap= 0.82507 test_roc= 0.79095 test_ap= 0.81770\n",
            "Epoch: 0071 train_loss= 0.51954 train_acc= 0.66875 val_roc= 0.82867 val_ap= 0.85375 test_roc= 0.82967 test_ap= 0.85943\n",
            "Epoch: 0081 train_loss= 0.49705 train_acc= 0.68400 val_roc= 0.84834 val_ap= 0.87605 test_roc= 0.84885 test_ap= 0.87698\n",
            "Epoch: 0091 train_loss= 0.48369 train_acc= 0.68764 val_roc= 0.86279 val_ap= 0.88892 test_roc= 0.85955 test_ap= 0.88771\n",
            "Epoch: 0101 train_loss= 0.47378 train_acc= 0.69419 val_roc= 0.87001 val_ap= 0.89449 test_roc= 0.86961 test_ap= 0.89634\n",
            "Epoch: 0111 train_loss= 0.46691 train_acc= 0.69873 val_roc= 0.88054 val_ap= 0.90231 test_roc= 0.87619 test_ap= 0.90154\n",
            "Epoch: 0121 train_loss= 0.46018 train_acc= 0.70849 val_roc= 0.89212 val_ap= 0.91180 test_roc= 0.88053 test_ap= 0.90563\n",
            "Epoch: 0131 train_loss= 0.45493 train_acc= 0.71281 val_roc= 0.89847 val_ap= 0.91857 test_roc= 0.88658 test_ap= 0.91107\n",
            "Epoch: 0141 train_loss= 0.45053 train_acc= 0.71939 val_roc= 0.90019 val_ap= 0.91941 test_roc= 0.88995 test_ap= 0.91355\n",
            "Epoch: 0151 train_loss= 0.44602 train_acc= 0.72591 val_roc= 0.90215 val_ap= 0.91992 test_roc= 0.89326 test_ap= 0.91534\n",
            "Epoch: 0161 train_loss= 0.44168 train_acc= 0.73149 val_roc= 0.90559 val_ap= 0.92272 test_roc= 0.89809 test_ap= 0.91853\n",
            "Epoch: 0171 train_loss= 0.43815 train_acc= 0.73773 val_roc= 0.90919 val_ap= 0.92658 test_roc= 0.90310 test_ap= 0.92180\n",
            "Epoch: 0181 train_loss= 0.43632 train_acc= 0.74340 val_roc= 0.91182 val_ap= 0.92775 test_roc= 0.90727 test_ap= 0.92518\n",
            "Epoch: 0191 train_loss= 0.43415 train_acc= 0.74532 val_roc= 0.91401 val_ap= 0.92783 test_roc= 0.90903 test_ap= 0.92644\n",
            "Epoch: 0201 train_loss= 0.43264 train_acc= 0.75043 val_roc= 0.91723 val_ap= 0.93002 test_roc= 0.90998 test_ap= 0.92770\n",
            "Epoch: 0211 train_loss= 0.43149 train_acc= 0.75077 val_roc= 0.92011 val_ap= 0.93256 test_roc= 0.91193 test_ap= 0.92936\n",
            "Epoch: 0221 train_loss= 0.42999 train_acc= 0.75571 val_roc= 0.92083 val_ap= 0.93289 test_roc= 0.91352 test_ap= 0.93110\n",
            "Epoch: 0231 train_loss= 0.42860 train_acc= 0.75772 val_roc= 0.92114 val_ap= 0.93339 test_roc= 0.91494 test_ap= 0.93294\n",
            "Epoch: 0241 train_loss= 0.42751 train_acc= 0.76104 val_roc= 0.92154 val_ap= 0.93373 test_roc= 0.91572 test_ap= 0.93413\n",
            "Epoch: 0251 train_loss= 0.42652 train_acc= 0.76244 val_roc= 0.92209 val_ap= 0.93536 test_roc= 0.91605 test_ap= 0.93455\n",
            "Epoch: 0261 train_loss= 0.42558 train_acc= 0.76263 val_roc= 0.92345 val_ap= 0.93602 test_roc= 0.91707 test_ap= 0.93529\n",
            "Epoch: 0271 train_loss= 0.42521 train_acc= 0.76511 val_roc= 0.92390 val_ap= 0.93649 test_roc= 0.91717 test_ap= 0.93578\n",
            "Epoch: 0281 train_loss= 0.42486 train_acc= 0.76519 val_roc= 0.92385 val_ap= 0.93651 test_roc= 0.91638 test_ap= 0.93503\n",
            "Epoch: 0291 train_loss= 0.42448 train_acc= 0.76779 val_roc= 0.92433 val_ap= 0.93681 test_roc= 0.91636 test_ap= 0.93509\n",
            "Epoch: 0301 train_loss= 0.42416 train_acc= 0.76885 val_roc= 0.92456 val_ap= 0.93707 test_roc= 0.91618 test_ap= 0.93542\n",
            "Epoch: 0311 train_loss= 0.42370 train_acc= 0.77010 val_roc= 0.92530 val_ap= 0.93794 test_roc= 0.91594 test_ap= 0.93514\n",
            "Epoch: 0321 train_loss= 0.42306 train_acc= 0.77251 val_roc= 0.92588 val_ap= 0.93774 test_roc= 0.91625 test_ap= 0.93566\n",
            "Epoch: 0331 train_loss= 0.42252 train_acc= 0.77328 val_roc= 0.92543 val_ap= 0.93718 test_roc= 0.91558 test_ap= 0.93527\n",
            "Epoch: 0341 train_loss= 0.42216 train_acc= 0.77510 val_roc= 0.92559 val_ap= 0.93797 test_roc= 0.91566 test_ap= 0.93523\n",
            "Epoch: 0351 train_loss= 0.42178 train_acc= 0.77605 val_roc= 0.92377 val_ap= 0.93737 test_roc= 0.91554 test_ap= 0.93535\n",
            "Epoch: 0361 train_loss= 0.42147 train_acc= 0.77717 val_roc= 0.92333 val_ap= 0.93677 test_roc= 0.91592 test_ap= 0.93569\n",
            "Epoch: 0371 train_loss= 0.42117 train_acc= 0.77908 val_roc= 0.92284 val_ap= 0.93659 test_roc= 0.91589 test_ap= 0.93569\n",
            "Epoch: 0381 train_loss= 0.42105 train_acc= 0.77876 val_roc= 0.92283 val_ap= 0.93662 test_roc= 0.91604 test_ap= 0.93583\n",
            "Epoch: 0391 train_loss= 0.42077 train_acc= 0.78101 val_roc= 0.92300 val_ap= 0.93628 test_roc= 0.91674 test_ap= 0.93599\n",
            "Epoch: 0401 train_loss= 0.42078 train_acc= 0.78144 val_roc= 0.92281 val_ap= 0.93573 test_roc= 0.91658 test_ap= 0.93560\n",
            "Epoch: 0411 train_loss= 0.42073 train_acc= 0.78394 val_roc= 0.92257 val_ap= 0.93575 test_roc= 0.91684 test_ap= 0.93558\n",
            "Epoch: 0421 train_loss= 0.42105 train_acc= 0.78446 val_roc= 0.92316 val_ap= 0.93679 test_roc= 0.91703 test_ap= 0.93574\n",
            "Epoch: 0431 train_loss= 0.42061 train_acc= 0.78522 val_roc= 0.92440 val_ap= 0.93734 test_roc= 0.91695 test_ap= 0.93581\n",
            "Epoch: 0441 train_loss= 0.42072 train_acc= 0.78760 val_roc= 0.92364 val_ap= 0.93669 test_roc= 0.91695 test_ap= 0.93571\n",
            "Epoch: 0451 train_loss= 0.42035 train_acc= 0.78781 val_roc= 0.92312 val_ap= 0.93621 test_roc= 0.91739 test_ap= 0.93611\n",
            "Epoch: 0461 train_loss= 0.42025 train_acc= 0.78915 val_roc= 0.92274 val_ap= 0.93543 test_roc= 0.91718 test_ap= 0.93570\n",
            "Epoch: 0471 train_loss= 0.42045 train_acc= 0.78956 val_roc= 0.92291 val_ap= 0.93607 test_roc= 0.91776 test_ap= 0.93605\n",
            "Epoch: 0481 train_loss= 0.42034 train_acc= 0.79145 val_roc= 0.92260 val_ap= 0.93551 test_roc= 0.91804 test_ap= 0.93601\n",
            "Epoch: 0491 train_loss= 0.42005 train_acc= 0.79222 val_roc= 0.92184 val_ap= 0.93475 test_roc= 0.91849 test_ap= 0.93604\n",
            "Best epoch (from validation):  326\n",
            "test_roc  0.9161700794659542\n",
            "test_ap  0.9357274785543598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iprNRxwFQfOd",
        "outputId": "bd43aa09-e7ca-47fe-da1c-7a8929461c97"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap, lambda = 0.4\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.4\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.90259 train_acc= 0.64199 val_roc= 0.66903 val_ap= 0.67100 test_roc= 0.68656 test_ap= 0.69007\n",
            "Epoch: 0011 train_loss= 0.84801 train_acc= 0.66758 val_roc= 0.76150 val_ap= 0.77715 test_roc= 0.75238 test_ap= 0.77144\n",
            "Epoch: 0021 train_loss= 0.73597 train_acc= 0.71688 val_roc= 0.72206 val_ap= 0.75274 test_roc= 0.74176 test_ap= 0.76784\n",
            "Epoch: 0031 train_loss= 0.68723 train_acc= 0.79369 val_roc= 0.71784 val_ap= 0.73967 test_roc= 0.73420 test_ap= 0.75464\n",
            "Epoch: 0041 train_loss= 0.63443 train_acc= 0.78718 val_roc= 0.70050 val_ap= 0.72766 test_roc= 0.69746 test_ap= 0.73251\n",
            "Epoch: 0051 train_loss= 0.59193 train_acc= 0.71634 val_roc= 0.75103 val_ap= 0.77250 test_roc= 0.73864 test_ap= 0.77175\n",
            "Epoch: 0061 train_loss= 0.55021 train_acc= 0.67051 val_roc= 0.81697 val_ap= 0.82637 test_roc= 0.80084 test_ap= 0.82237\n",
            "Epoch: 0071 train_loss= 0.52175 train_acc= 0.68055 val_roc= 0.84324 val_ap= 0.85513 test_roc= 0.84329 test_ap= 0.86536\n",
            "Epoch: 0081 train_loss= 0.49747 train_acc= 0.67278 val_roc= 0.85767 val_ap= 0.86827 test_roc= 0.86970 test_ap= 0.89063\n",
            "Epoch: 0091 train_loss= 0.48343 train_acc= 0.68075 val_roc= 0.86325 val_ap= 0.87630 test_roc= 0.87903 test_ap= 0.90003\n",
            "Epoch: 0101 train_loss= 0.47303 train_acc= 0.69342 val_roc= 0.87166 val_ap= 0.88991 test_roc= 0.88562 test_ap= 0.90937\n",
            "Epoch: 0111 train_loss= 0.46586 train_acc= 0.69607 val_roc= 0.87465 val_ap= 0.89643 test_roc= 0.88943 test_ap= 0.91385\n",
            "Epoch: 0121 train_loss= 0.46084 train_acc= 0.70604 val_roc= 0.87638 val_ap= 0.90048 test_roc= 0.89005 test_ap= 0.91495\n",
            "Epoch: 0131 train_loss= 0.45677 train_acc= 0.70827 val_roc= 0.87880 val_ap= 0.90511 test_roc= 0.89021 test_ap= 0.91507\n",
            "Epoch: 0141 train_loss= 0.45278 train_acc= 0.71437 val_roc= 0.88089 val_ap= 0.90774 test_roc= 0.89292 test_ap= 0.91722\n",
            "Epoch: 0151 train_loss= 0.44926 train_acc= 0.72029 val_roc= 0.88152 val_ap= 0.90897 test_roc= 0.89484 test_ap= 0.91938\n",
            "Epoch: 0161 train_loss= 0.44545 train_acc= 0.72408 val_roc= 0.88392 val_ap= 0.91171 test_roc= 0.89664 test_ap= 0.92147\n",
            "Epoch: 0171 train_loss= 0.44193 train_acc= 0.72958 val_roc= 0.88576 val_ap= 0.91348 test_roc= 0.89913 test_ap= 0.92387\n",
            "Epoch: 0181 train_loss= 0.43982 train_acc= 0.73480 val_roc= 0.88758 val_ap= 0.91511 test_roc= 0.90098 test_ap= 0.92517\n",
            "Epoch: 0191 train_loss= 0.43699 train_acc= 0.73860 val_roc= 0.89034 val_ap= 0.91656 test_roc= 0.90408 test_ap= 0.92659\n",
            "Epoch: 0201 train_loss= 0.43553 train_acc= 0.74233 val_roc= 0.89250 val_ap= 0.91847 test_roc= 0.90702 test_ap= 0.92831\n",
            "Epoch: 0211 train_loss= 0.43466 train_acc= 0.74405 val_roc= 0.89401 val_ap= 0.92036 test_roc= 0.90805 test_ap= 0.92886\n",
            "Epoch: 0221 train_loss= 0.43311 train_acc= 0.74813 val_roc= 0.89595 val_ap= 0.92296 test_roc= 0.90958 test_ap= 0.92986\n",
            "Epoch: 0231 train_loss= 0.43184 train_acc= 0.74871 val_roc= 0.89818 val_ap= 0.92506 test_roc= 0.91068 test_ap= 0.93070\n",
            "Epoch: 0241 train_loss= 0.43098 train_acc= 0.75121 val_roc= 0.89913 val_ap= 0.92594 test_roc= 0.91149 test_ap= 0.93134\n",
            "Epoch: 0251 train_loss= 0.42987 train_acc= 0.75483 val_roc= 0.90027 val_ap= 0.92677 test_roc= 0.91218 test_ap= 0.93172\n",
            "Epoch: 0261 train_loss= 0.42871 train_acc= 0.75601 val_roc= 0.90152 val_ap= 0.92736 test_roc= 0.91199 test_ap= 0.93198\n",
            "Epoch: 0271 train_loss= 0.42820 train_acc= 0.75841 val_roc= 0.90289 val_ap= 0.92899 test_roc= 0.91160 test_ap= 0.93198\n",
            "Epoch: 0281 train_loss= 0.42735 train_acc= 0.75991 val_roc= 0.90334 val_ap= 0.92912 test_roc= 0.91191 test_ap= 0.93247\n",
            "Epoch: 0291 train_loss= 0.42623 train_acc= 0.76273 val_roc= 0.90384 val_ap= 0.92991 test_roc= 0.91142 test_ap= 0.93243\n",
            "Epoch: 0301 train_loss= 0.42525 train_acc= 0.76392 val_roc= 0.90493 val_ap= 0.93093 test_roc= 0.91182 test_ap= 0.93279\n",
            "Epoch: 0311 train_loss= 0.42471 train_acc= 0.76618 val_roc= 0.90551 val_ap= 0.93108 test_roc= 0.91163 test_ap= 0.93257\n",
            "Epoch: 0321 train_loss= 0.42393 train_acc= 0.76621 val_roc= 0.90595 val_ap= 0.93126 test_roc= 0.91143 test_ap= 0.93259\n",
            "Epoch: 0331 train_loss= 0.42343 train_acc= 0.76740 val_roc= 0.90650 val_ap= 0.93121 test_roc= 0.91174 test_ap= 0.93253\n",
            "Epoch: 0341 train_loss= 0.42346 train_acc= 0.76905 val_roc= 0.90818 val_ap= 0.93220 test_roc= 0.91275 test_ap= 0.93275\n",
            "Epoch: 0351 train_loss= 0.42298 train_acc= 0.76901 val_roc= 0.90765 val_ap= 0.93179 test_roc= 0.91293 test_ap= 0.93312\n",
            "Epoch: 0361 train_loss= 0.42280 train_acc= 0.77135 val_roc= 0.90781 val_ap= 0.93176 test_roc= 0.91286 test_ap= 0.93337\n",
            "Epoch: 0371 train_loss= 0.42243 train_acc= 0.77316 val_roc= 0.90817 val_ap= 0.93240 test_roc= 0.91294 test_ap= 0.93330\n",
            "Epoch: 0381 train_loss= 0.42226 train_acc= 0.77222 val_roc= 0.90850 val_ap= 0.93231 test_roc= 0.91311 test_ap= 0.93336\n",
            "Epoch: 0391 train_loss= 0.42216 train_acc= 0.77459 val_roc= 0.90820 val_ap= 0.93168 test_roc= 0.91310 test_ap= 0.93321\n",
            "Epoch: 0401 train_loss= 0.42190 train_acc= 0.77602 val_roc= 0.90908 val_ap= 0.93206 test_roc= 0.91310 test_ap= 0.93312\n",
            "Epoch: 0411 train_loss= 0.42160 train_acc= 0.77788 val_roc= 0.90885 val_ap= 0.93144 test_roc= 0.91313 test_ap= 0.93290\n",
            "Epoch: 0421 train_loss= 0.42132 train_acc= 0.77971 val_roc= 0.90821 val_ap= 0.93171 test_roc= 0.91274 test_ap= 0.93295\n",
            "Epoch: 0431 train_loss= 0.42101 train_acc= 0.77984 val_roc= 0.90938 val_ap= 0.93278 test_roc= 0.91295 test_ap= 0.93297\n",
            "Epoch: 0441 train_loss= 0.42067 train_acc= 0.78121 val_roc= 0.90916 val_ap= 0.93213 test_roc= 0.91354 test_ap= 0.93332\n",
            "Epoch: 0451 train_loss= 0.42038 train_acc= 0.78137 val_roc= 0.90880 val_ap= 0.93196 test_roc= 0.91376 test_ap= 0.93333\n",
            "Epoch: 0461 train_loss= 0.42017 train_acc= 0.78294 val_roc= 0.90776 val_ap= 0.93159 test_roc= 0.91337 test_ap= 0.93267\n",
            "Epoch: 0471 train_loss= 0.42011 train_acc= 0.78344 val_roc= 0.90809 val_ap= 0.93218 test_roc= 0.91398 test_ap= 0.93323\n",
            "Epoch: 0481 train_loss= 0.41991 train_acc= 0.78480 val_roc= 0.90929 val_ap= 0.93279 test_roc= 0.91373 test_ap= 0.93309\n",
            "Epoch: 0491 train_loss= 0.41976 train_acc= 0.78526 val_roc= 0.90850 val_ap= 0.93226 test_roc= 0.91365 test_ap= 0.93283\n",
            "Best epoch (from validation):  431\n",
            "test_roc  0.9130663344483292\n",
            "test_ap  0.932945488568201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnoZUpOeQfWJ",
        "outputId": "23b6d0be-5098-4695-890a-29fee60dce26"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lambda=0.5\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.5\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.80882 train_acc= 0.69771 val_roc= 0.67574 val_ap= 0.66588 test_roc= 0.67361 test_ap= 0.67749\n",
            "Epoch: 0011 train_loss= 0.78130 train_acc= 0.72602 val_roc= 0.74086 val_ap= 0.76179 test_roc= 0.77725 test_ap= 0.79557\n",
            "Epoch: 0021 train_loss= 0.73778 train_acc= 0.77119 val_roc= 0.70055 val_ap= 0.73771 test_roc= 0.74317 test_ap= 0.77417\n",
            "Epoch: 0031 train_loss= 0.69061 train_acc= 0.74512 val_roc= 0.70733 val_ap= 0.73260 test_roc= 0.73703 test_ap= 0.76095\n",
            "Epoch: 0041 train_loss= 0.63045 train_acc= 0.76405 val_roc= 0.70089 val_ap= 0.73006 test_roc= 0.68503 test_ap= 0.72430\n",
            "Epoch: 0051 train_loss= 0.60062 train_acc= 0.69555 val_roc= 0.74014 val_ap= 0.76289 test_roc= 0.71007 test_ap= 0.74698\n",
            "Epoch: 0061 train_loss= 0.56031 train_acc= 0.69319 val_roc= 0.79488 val_ap= 0.81152 test_roc= 0.77057 test_ap= 0.79144\n",
            "Epoch: 0071 train_loss= 0.54066 train_acc= 0.66329 val_roc= 0.81677 val_ap= 0.82781 test_roc= 0.79141 test_ap= 0.81030\n",
            "Epoch: 0081 train_loss= 0.51771 train_acc= 0.67934 val_roc= 0.84288 val_ap= 0.85829 test_roc= 0.82251 test_ap= 0.84297\n",
            "Epoch: 0091 train_loss= 0.49976 train_acc= 0.67472 val_roc= 0.85645 val_ap= 0.86700 test_roc= 0.84553 test_ap= 0.86366\n",
            "Epoch: 0101 train_loss= 0.48648 train_acc= 0.68429 val_roc= 0.86965 val_ap= 0.88256 test_roc= 0.86455 test_ap= 0.88258\n",
            "Epoch: 0111 train_loss= 0.47593 train_acc= 0.69338 val_roc= 0.87743 val_ap= 0.89228 test_roc= 0.87477 test_ap= 0.89542\n",
            "Epoch: 0121 train_loss= 0.46761 train_acc= 0.69984 val_roc= 0.88538 val_ap= 0.89742 test_roc= 0.88742 test_ap= 0.90860\n",
            "Epoch: 0131 train_loss= 0.46144 train_acc= 0.70745 val_roc= 0.88859 val_ap= 0.90372 test_roc= 0.89449 test_ap= 0.91691\n",
            "Epoch: 0141 train_loss= 0.45667 train_acc= 0.71118 val_roc= 0.89293 val_ap= 0.91045 test_roc= 0.89853 test_ap= 0.92084\n",
            "Epoch: 0151 train_loss= 0.45252 train_acc= 0.71847 val_roc= 0.89615 val_ap= 0.91387 test_roc= 0.89905 test_ap= 0.92152\n",
            "Epoch: 0161 train_loss= 0.44893 train_acc= 0.72294 val_roc= 0.90126 val_ap= 0.91827 test_roc= 0.90161 test_ap= 0.92310\n",
            "Epoch: 0171 train_loss= 0.44546 train_acc= 0.72755 val_roc= 0.90377 val_ap= 0.92133 test_roc= 0.90262 test_ap= 0.92334\n",
            "Epoch: 0181 train_loss= 0.44252 train_acc= 0.73308 val_roc= 0.90538 val_ap= 0.92385 test_roc= 0.90345 test_ap= 0.92334\n",
            "Epoch: 0191 train_loss= 0.43945 train_acc= 0.73789 val_roc= 0.90467 val_ap= 0.92406 test_roc= 0.90513 test_ap= 0.92407\n",
            "Epoch: 0201 train_loss= 0.43659 train_acc= 0.74029 val_roc= 0.90249 val_ap= 0.92339 test_roc= 0.90525 test_ap= 0.92374\n",
            "Epoch: 0211 train_loss= 0.43538 train_acc= 0.74268 val_roc= 0.89994 val_ap= 0.92187 test_roc= 0.90575 test_ap= 0.92418\n",
            "Epoch: 0221 train_loss= 0.43400 train_acc= 0.74450 val_roc= 0.90144 val_ap= 0.92326 test_roc= 0.90777 test_ap= 0.92621\n",
            "Epoch: 0231 train_loss= 0.43290 train_acc= 0.74624 val_roc= 0.90113 val_ap= 0.92335 test_roc= 0.90871 test_ap= 0.92681\n",
            "Epoch: 0241 train_loss= 0.43247 train_acc= 0.75026 val_roc= 0.90198 val_ap= 0.92407 test_roc= 0.90945 test_ap= 0.92721\n",
            "Epoch: 0251 train_loss= 0.43182 train_acc= 0.75049 val_roc= 0.90240 val_ap= 0.92421 test_roc= 0.91028 test_ap= 0.92749\n",
            "Epoch: 0261 train_loss= 0.43129 train_acc= 0.75180 val_roc= 0.90447 val_ap= 0.92522 test_roc= 0.91187 test_ap= 0.92853\n",
            "Epoch: 0271 train_loss= 0.43104 train_acc= 0.75376 val_roc= 0.90601 val_ap= 0.92553 test_roc= 0.91346 test_ap= 0.92992\n",
            "Epoch: 0281 train_loss= 0.43015 train_acc= 0.75485 val_roc= 0.90684 val_ap= 0.92620 test_roc= 0.91392 test_ap= 0.92998\n",
            "Epoch: 0291 train_loss= 0.42978 train_acc= 0.75665 val_roc= 0.90752 val_ap= 0.92722 test_roc= 0.91461 test_ap= 0.93056\n",
            "Epoch: 0301 train_loss= 0.42849 train_acc= 0.75795 val_roc= 0.90870 val_ap= 0.92778 test_roc= 0.91541 test_ap= 0.93122\n",
            "Epoch: 0311 train_loss= 0.42782 train_acc= 0.76121 val_roc= 0.90963 val_ap= 0.92839 test_roc= 0.91572 test_ap= 0.93147\n",
            "Epoch: 0321 train_loss= 0.42650 train_acc= 0.76173 val_roc= 0.91028 val_ap= 0.92796 test_roc= 0.91559 test_ap= 0.93197\n",
            "Epoch: 0331 train_loss= 0.42560 train_acc= 0.76374 val_roc= 0.91103 val_ap= 0.92784 test_roc= 0.91419 test_ap= 0.93108\n",
            "Epoch: 0341 train_loss= 0.42477 train_acc= 0.76580 val_roc= 0.91235 val_ap= 0.92858 test_roc= 0.91368 test_ap= 0.93115\n",
            "Epoch: 0351 train_loss= 0.42388 train_acc= 0.76634 val_roc= 0.91420 val_ap= 0.93018 test_roc= 0.91354 test_ap= 0.93139\n",
            "Epoch: 0361 train_loss= 0.42356 train_acc= 0.76948 val_roc= 0.91605 val_ap= 0.93157 test_roc= 0.91273 test_ap= 0.93115\n",
            "Epoch: 0371 train_loss= 0.42268 train_acc= 0.76953 val_roc= 0.91725 val_ap= 0.93255 test_roc= 0.91285 test_ap= 0.93155\n",
            "Epoch: 0381 train_loss= 0.42190 train_acc= 0.77010 val_roc= 0.91710 val_ap= 0.93280 test_roc= 0.91329 test_ap= 0.93211\n",
            "Epoch: 0391 train_loss= 0.42143 train_acc= 0.77231 val_roc= 0.91722 val_ap= 0.93353 test_roc= 0.91301 test_ap= 0.93169\n",
            "Epoch: 0401 train_loss= 0.42125 train_acc= 0.77374 val_roc= 0.91771 val_ap= 0.93427 test_roc= 0.91266 test_ap= 0.93157\n",
            "Epoch: 0411 train_loss= 0.42078 train_acc= 0.77423 val_roc= 0.91849 val_ap= 0.93466 test_roc= 0.91315 test_ap= 0.93194\n",
            "Epoch: 0421 train_loss= 0.42072 train_acc= 0.77540 val_roc= 0.91897 val_ap= 0.93547 test_roc= 0.91356 test_ap= 0.93229\n",
            "Epoch: 0431 train_loss= 0.42062 train_acc= 0.77499 val_roc= 0.91936 val_ap= 0.93584 test_roc= 0.91326 test_ap= 0.93203\n",
            "Epoch: 0441 train_loss= 0.42057 train_acc= 0.77730 val_roc= 0.91994 val_ap= 0.93645 test_roc= 0.91316 test_ap= 0.93197\n",
            "Epoch: 0451 train_loss= 0.42025 train_acc= 0.77863 val_roc= 0.92041 val_ap= 0.93687 test_roc= 0.91315 test_ap= 0.93181\n",
            "Epoch: 0461 train_loss= 0.42033 train_acc= 0.77870 val_roc= 0.92087 val_ap= 0.93691 test_roc= 0.91328 test_ap= 0.93224\n",
            "Epoch: 0471 train_loss= 0.42013 train_acc= 0.78035 val_roc= 0.92121 val_ap= 0.93748 test_roc= 0.91275 test_ap= 0.93207\n",
            "Epoch: 0481 train_loss= 0.41994 train_acc= 0.78136 val_roc= 0.92166 val_ap= 0.93732 test_roc= 0.91243 test_ap= 0.93170\n",
            "Epoch: 0491 train_loss= 0.41994 train_acc= 0.78219 val_roc= 0.92221 val_ap= 0.93761 test_roc= 0.91248 test_ap= 0.93180\n",
            "Best epoch (from validation):  490\n",
            "test_roc  0.9124794313881518\n",
            "test_ap  0.9318048857012708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SarWeGwLQfb9",
        "outputId": "fbf7876e-8846-44cb-cbd9-90ef47d53413"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lambda=0.6\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.6\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.76083 train_acc= 0.79032 val_roc= 0.61789 val_ap= 0.62881 test_roc= 0.63921 test_ap= 0.65157\n",
            "Epoch: 0011 train_loss= 0.74918 train_acc= 0.81098 val_roc= 0.71679 val_ap= 0.74306 test_roc= 0.75284 test_ap= 0.78539\n",
            "Epoch: 0021 train_loss= 0.73716 train_acc= 0.84528 val_roc= 0.69972 val_ap= 0.74628 test_roc= 0.73555 test_ap= 0.77489\n",
            "Epoch: 0031 train_loss= 0.70289 train_acc= 0.79552 val_roc= 0.70044 val_ap= 0.73546 test_roc= 0.72951 test_ap= 0.76502\n",
            "Epoch: 0041 train_loss= 0.65214 train_acc= 0.77438 val_roc= 0.70247 val_ap= 0.72256 test_roc= 0.70108 test_ap= 0.73918\n",
            "Epoch: 0051 train_loss= 0.61787 train_acc= 0.73457 val_roc= 0.68276 val_ap= 0.71691 test_roc= 0.69032 test_ap= 0.73562\n",
            "Epoch: 0061 train_loss= 0.58032 train_acc= 0.71118 val_roc= 0.73422 val_ap= 0.75769 test_roc= 0.75306 test_ap= 0.79217\n",
            "Epoch: 0071 train_loss= 0.54352 train_acc= 0.68512 val_roc= 0.76657 val_ap= 0.78204 test_roc= 0.80084 test_ap= 0.82472\n",
            "Epoch: 0081 train_loss= 0.52123 train_acc= 0.68107 val_roc= 0.79800 val_ap= 0.81411 test_roc= 0.83385 test_ap= 0.86330\n",
            "Epoch: 0091 train_loss= 0.50268 train_acc= 0.68487 val_roc= 0.82670 val_ap= 0.84270 test_roc= 0.85787 test_ap= 0.88559\n",
            "Epoch: 0101 train_loss= 0.48994 train_acc= 0.68641 val_roc= 0.84064 val_ap= 0.85954 test_roc= 0.86748 test_ap= 0.89267\n",
            "Epoch: 0111 train_loss= 0.48215 train_acc= 0.69222 val_roc= 0.85376 val_ap= 0.87892 test_roc= 0.87406 test_ap= 0.89874\n",
            "Epoch: 0121 train_loss= 0.47515 train_acc= 0.69907 val_roc= 0.86211 val_ap= 0.88880 test_roc= 0.88008 test_ap= 0.90460\n",
            "Epoch: 0131 train_loss= 0.47129 train_acc= 0.69962 val_roc= 0.87136 val_ap= 0.89853 test_roc= 0.88537 test_ap= 0.90942\n",
            "Epoch: 0141 train_loss= 0.46641 train_acc= 0.70381 val_roc= 0.88048 val_ap= 0.90837 test_roc= 0.88957 test_ap= 0.91302\n",
            "Epoch: 0151 train_loss= 0.46231 train_acc= 0.70922 val_roc= 0.88384 val_ap= 0.91177 test_roc= 0.89302 test_ap= 0.91615\n",
            "Epoch: 0161 train_loss= 0.45822 train_acc= 0.71466 val_roc= 0.88690 val_ap= 0.91409 test_roc= 0.89598 test_ap= 0.91807\n",
            "Epoch: 0171 train_loss= 0.45411 train_acc= 0.71977 val_roc= 0.88577 val_ap= 0.91332 test_roc= 0.89708 test_ap= 0.91802\n",
            "Epoch: 0181 train_loss= 0.45218 train_acc= 0.72306 val_roc= 0.88479 val_ap= 0.91188 test_roc= 0.89739 test_ap= 0.91787\n",
            "Epoch: 0191 train_loss= 0.44965 train_acc= 0.72627 val_roc= 0.88800 val_ap= 0.91382 test_roc= 0.89979 test_ap= 0.91973\n",
            "Epoch: 0201 train_loss= 0.44786 train_acc= 0.72854 val_roc= 0.89070 val_ap= 0.91560 test_roc= 0.90135 test_ap= 0.92080\n",
            "Epoch: 0211 train_loss= 0.44673 train_acc= 0.72993 val_roc= 0.89397 val_ap= 0.91721 test_roc= 0.90273 test_ap= 0.92143\n",
            "Epoch: 0221 train_loss= 0.44514 train_acc= 0.73283 val_roc= 0.89676 val_ap= 0.91907 test_roc= 0.90425 test_ap= 0.92218\n",
            "Epoch: 0231 train_loss= 0.44302 train_acc= 0.73662 val_roc= 0.89900 val_ap= 0.92066 test_roc= 0.90555 test_ap= 0.92340\n",
            "Epoch: 0241 train_loss= 0.44177 train_acc= 0.74198 val_roc= 0.90097 val_ap= 0.92146 test_roc= 0.90650 test_ap= 0.92466\n",
            "Epoch: 0251 train_loss= 0.44023 train_acc= 0.74455 val_roc= 0.90168 val_ap= 0.92190 test_roc= 0.90714 test_ap= 0.92564\n",
            "Epoch: 0261 train_loss= 0.43874 train_acc= 0.74429 val_roc= 0.90222 val_ap= 0.92294 test_roc= 0.90757 test_ap= 0.92639\n",
            "Epoch: 0271 train_loss= 0.43755 train_acc= 0.74718 val_roc= 0.90392 val_ap= 0.92454 test_roc= 0.90789 test_ap= 0.92718\n",
            "Epoch: 0281 train_loss= 0.43634 train_acc= 0.74971 val_roc= 0.90470 val_ap= 0.92492 test_roc= 0.90801 test_ap= 0.92778\n",
            "Epoch: 0291 train_loss= 0.43477 train_acc= 0.75253 val_roc= 0.90516 val_ap= 0.92522 test_roc= 0.90743 test_ap= 0.92773\n",
            "Epoch: 0301 train_loss= 0.43330 train_acc= 0.75438 val_roc= 0.90477 val_ap= 0.92482 test_roc= 0.90741 test_ap= 0.92814\n",
            "Epoch: 0311 train_loss= 0.43242 train_acc= 0.75628 val_roc= 0.90509 val_ap= 0.92459 test_roc= 0.90776 test_ap= 0.92853\n",
            "Epoch: 0321 train_loss= 0.43096 train_acc= 0.75755 val_roc= 0.90603 val_ap= 0.92446 test_roc= 0.90788 test_ap= 0.92892\n",
            "Epoch: 0331 train_loss= 0.42988 train_acc= 0.75964 val_roc= 0.90731 val_ap= 0.92526 test_roc= 0.90739 test_ap= 0.92846\n",
            "Epoch: 0341 train_loss= 0.42903 train_acc= 0.76061 val_roc= 0.90937 val_ap= 0.92668 test_roc= 0.90779 test_ap= 0.92844\n",
            "Epoch: 0351 train_loss= 0.42819 train_acc= 0.76207 val_roc= 0.91097 val_ap= 0.92814 test_roc= 0.90798 test_ap= 0.92858\n",
            "Epoch: 0361 train_loss= 0.42780 train_acc= 0.76309 val_roc= 0.91249 val_ap= 0.92940 test_roc= 0.90792 test_ap= 0.92883\n",
            "Epoch: 0371 train_loss= 0.42684 train_acc= 0.76484 val_roc= 0.91454 val_ap= 0.93105 test_roc= 0.90758 test_ap= 0.92856\n",
            "Epoch: 0381 train_loss= 0.42664 train_acc= 0.76499 val_roc= 0.91649 val_ap= 0.93254 test_roc= 0.90747 test_ap= 0.92853\n",
            "Epoch: 0391 train_loss= 0.42578 train_acc= 0.76616 val_roc= 0.91756 val_ap= 0.93303 test_roc= 0.90703 test_ap= 0.92829\n",
            "Epoch: 0401 train_loss= 0.42557 train_acc= 0.76875 val_roc= 0.91837 val_ap= 0.93370 test_roc= 0.90665 test_ap= 0.92791\n",
            "Epoch: 0411 train_loss= 0.42481 train_acc= 0.76865 val_roc= 0.91863 val_ap= 0.93425 test_roc= 0.90659 test_ap= 0.92794\n",
            "Epoch: 0421 train_loss= 0.42410 train_acc= 0.77218 val_roc= 0.91910 val_ap= 0.93460 test_roc= 0.90654 test_ap= 0.92808\n",
            "Epoch: 0431 train_loss= 0.42383 train_acc= 0.77335 val_roc= 0.91969 val_ap= 0.93463 test_roc= 0.90611 test_ap= 0.92787\n",
            "Epoch: 0441 train_loss= 0.42331 train_acc= 0.77482 val_roc= 0.91952 val_ap= 0.93435 test_roc= 0.90576 test_ap= 0.92744\n",
            "Epoch: 0451 train_loss= 0.42277 train_acc= 0.77475 val_roc= 0.91895 val_ap= 0.93438 test_roc= 0.90566 test_ap= 0.92730\n",
            "Epoch: 0461 train_loss= 0.42252 train_acc= 0.77583 val_roc= 0.91861 val_ap= 0.93425 test_roc= 0.90593 test_ap= 0.92761\n",
            "Epoch: 0471 train_loss= 0.42233 train_acc= 0.77697 val_roc= 0.91879 val_ap= 0.93472 test_roc= 0.90572 test_ap= 0.92741\n",
            "Epoch: 0481 train_loss= 0.42222 train_acc= 0.77874 val_roc= 0.91983 val_ap= 0.93485 test_roc= 0.90568 test_ap= 0.92706\n",
            "Epoch: 0491 train_loss= 0.42173 train_acc= 0.78036 val_roc= 0.92041 val_ap= 0.93496 test_roc= 0.90538 test_ap= 0.92679\n",
            "Best epoch (from validation):  489\n",
            "test_roc  0.9053861858142289\n",
            "test_ap  0.9267684146642265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcKuTE2QQpGa",
        "outputId": "077a109e-919e-4a76-935b-2349051ef8b2"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lambda=0.7\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.7\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.74433 train_acc= 0.91735 val_roc= 0.63390 val_ap= 0.65983 test_roc= 0.62630 test_ap= 0.64848\n",
            "Epoch: 0011 train_loss= 0.74360 train_acc= 0.88814 val_roc= 0.67291 val_ap= 0.71574 test_roc= 0.69969 test_ap= 0.73603\n",
            "Epoch: 0021 train_loss= 0.74001 train_acc= 0.85556 val_roc= 0.67423 val_ap= 0.71484 test_roc= 0.69924 test_ap= 0.73492\n",
            "Epoch: 0031 train_loss= 0.72028 train_acc= 0.84558 val_roc= 0.68052 val_ap= 0.71214 test_roc= 0.71236 test_ap= 0.73734\n",
            "Epoch: 0041 train_loss= 0.66888 train_acc= 0.79960 val_roc= 0.68775 val_ap= 0.71513 test_roc= 0.70237 test_ap= 0.72873\n",
            "Epoch: 0051 train_loss= 0.63700 train_acc= 0.71768 val_roc= 0.69202 val_ap= 0.72161 test_roc= 0.68409 test_ap= 0.71991\n",
            "Epoch: 0061 train_loss= 0.61405 train_acc= 0.68762 val_roc= 0.72816 val_ap= 0.73915 test_roc= 0.73820 test_ap= 0.75818\n",
            "Epoch: 0071 train_loss= 0.60483 train_acc= 0.67016 val_roc= 0.74384 val_ap= 0.75496 test_roc= 0.75840 test_ap= 0.77508\n",
            "Epoch: 0081 train_loss= 0.59912 train_acc= 0.66772 val_roc= 0.75434 val_ap= 0.76729 test_roc= 0.76034 test_ap= 0.77583\n",
            "Epoch: 0091 train_loss= 0.59036 train_acc= 0.65586 val_roc= 0.74603 val_ap= 0.74281 test_roc= 0.73176 test_ap= 0.71697\n",
            "Epoch: 0101 train_loss= 0.57230 train_acc= 0.67010 val_roc= 0.76492 val_ap= 0.73893 test_roc= 0.73626 test_ap= 0.70954\n",
            "Epoch: 0111 train_loss= 0.54472 train_acc= 0.67925 val_roc= 0.80078 val_ap= 0.78121 test_roc= 0.76583 test_ap= 0.76134\n",
            "Epoch: 0121 train_loss= 0.52685 train_acc= 0.67946 val_roc= 0.83455 val_ap= 0.82944 test_roc= 0.80947 test_ap= 0.82073\n",
            "Epoch: 0131 train_loss= 0.51106 train_acc= 0.68338 val_roc= 0.84684 val_ap= 0.85459 test_roc= 0.82812 test_ap= 0.84081\n",
            "Epoch: 0141 train_loss= 0.50305 train_acc= 0.68000 val_roc= 0.85551 val_ap= 0.86388 test_roc= 0.84998 test_ap= 0.86632\n",
            "Epoch: 0151 train_loss= 0.49373 train_acc= 0.68202 val_roc= 0.85673 val_ap= 0.86741 test_roc= 0.86436 test_ap= 0.88046\n",
            "Epoch: 0161 train_loss= 0.48785 train_acc= 0.68672 val_roc= 0.85959 val_ap= 0.87163 test_roc= 0.87373 test_ap= 0.88871\n",
            "Epoch: 0171 train_loss= 0.48228 train_acc= 0.69020 val_roc= 0.86425 val_ap= 0.87806 test_roc= 0.88155 test_ap= 0.89484\n",
            "Epoch: 0181 train_loss= 0.47810 train_acc= 0.69771 val_roc= 0.86818 val_ap= 0.88251 test_roc= 0.88859 test_ap= 0.90118\n",
            "Epoch: 0191 train_loss= 0.47335 train_acc= 0.69969 val_roc= 0.87095 val_ap= 0.88562 test_roc= 0.89179 test_ap= 0.90420\n",
            "Epoch: 0201 train_loss= 0.46955 train_acc= 0.70377 val_roc= 0.87311 val_ap= 0.88983 test_roc= 0.89219 test_ap= 0.90547\n",
            "Epoch: 0211 train_loss= 0.46676 train_acc= 0.70897 val_roc= 0.87536 val_ap= 0.89346 test_roc= 0.89159 test_ap= 0.90642\n",
            "Epoch: 0221 train_loss= 0.46367 train_acc= 0.71249 val_roc= 0.87948 val_ap= 0.89909 test_roc= 0.89435 test_ap= 0.90862\n",
            "Epoch: 0231 train_loss= 0.46157 train_acc= 0.71340 val_roc= 0.88081 val_ap= 0.90191 test_roc= 0.89485 test_ap= 0.90883\n",
            "Epoch: 0241 train_loss= 0.46000 train_acc= 0.71486 val_roc= 0.88161 val_ap= 0.90350 test_roc= 0.89720 test_ap= 0.91138\n",
            "Epoch: 0251 train_loss= 0.45780 train_acc= 0.71600 val_roc= 0.88303 val_ap= 0.90561 test_roc= 0.89975 test_ap= 0.91425\n",
            "Epoch: 0261 train_loss= 0.45493 train_acc= 0.71807 val_roc= 0.88499 val_ap= 0.90718 test_roc= 0.90250 test_ap= 0.91715\n",
            "Epoch: 0271 train_loss= 0.45302 train_acc= 0.72336 val_roc= 0.88885 val_ap= 0.91084 test_roc= 0.90445 test_ap= 0.91964\n",
            "Epoch: 0281 train_loss= 0.45008 train_acc= 0.72656 val_roc= 0.89352 val_ap= 0.91552 test_roc= 0.90569 test_ap= 0.92132\n",
            "Epoch: 0291 train_loss= 0.44717 train_acc= 0.73093 val_roc= 0.89663 val_ap= 0.91857 test_roc= 0.90504 test_ap= 0.92157\n",
            "Epoch: 0301 train_loss= 0.44485 train_acc= 0.73291 val_roc= 0.89910 val_ap= 0.92083 test_roc= 0.90359 test_ap= 0.92077\n",
            "Epoch: 0311 train_loss= 0.44335 train_acc= 0.73675 val_roc= 0.90023 val_ap= 0.92199 test_roc= 0.90243 test_ap= 0.92019\n",
            "Epoch: 0321 train_loss= 0.44203 train_acc= 0.73724 val_roc= 0.90179 val_ap= 0.92272 test_roc= 0.90276 test_ap= 0.92062\n",
            "Epoch: 0331 train_loss= 0.44058 train_acc= 0.74031 val_roc= 0.90301 val_ap= 0.92327 test_roc= 0.90330 test_ap= 0.92091\n",
            "Epoch: 0341 train_loss= 0.44006 train_acc= 0.74220 val_roc= 0.90449 val_ap= 0.92380 test_roc= 0.90354 test_ap= 0.92068\n",
            "Epoch: 0351 train_loss= 0.43881 train_acc= 0.74340 val_roc= 0.90581 val_ap= 0.92461 test_roc= 0.90340 test_ap= 0.92030\n",
            "Epoch: 0361 train_loss= 0.43802 train_acc= 0.74567 val_roc= 0.90691 val_ap= 0.92508 test_roc= 0.90416 test_ap= 0.92131\n",
            "Epoch: 0371 train_loss= 0.43715 train_acc= 0.74622 val_roc= 0.90916 val_ap= 0.92658 test_roc= 0.90496 test_ap= 0.92198\n",
            "Epoch: 0381 train_loss= 0.43638 train_acc= 0.74699 val_roc= 0.91009 val_ap= 0.92757 test_roc= 0.90512 test_ap= 0.92214\n",
            "Epoch: 0391 train_loss= 0.43525 train_acc= 0.74910 val_roc= 0.91148 val_ap= 0.92846 test_roc= 0.90522 test_ap= 0.92232\n",
            "Epoch: 0401 train_loss= 0.43508 train_acc= 0.75044 val_roc= 0.91256 val_ap= 0.92906 test_roc= 0.90595 test_ap= 0.92317\n",
            "Epoch: 0411 train_loss= 0.43391 train_acc= 0.75269 val_roc= 0.91356 val_ap= 0.93005 test_roc= 0.90636 test_ap= 0.92348\n",
            "Epoch: 0421 train_loss= 0.43369 train_acc= 0.75352 val_roc= 0.91506 val_ap= 0.93128 test_roc= 0.90656 test_ap= 0.92400\n",
            "Epoch: 0431 train_loss= 0.43303 train_acc= 0.75448 val_roc= 0.91529 val_ap= 0.93136 test_roc= 0.90676 test_ap= 0.92437\n",
            "Epoch: 0441 train_loss= 0.43241 train_acc= 0.75769 val_roc= 0.91540 val_ap= 0.93131 test_roc= 0.90661 test_ap= 0.92441\n",
            "Epoch: 0451 train_loss= 0.43166 train_acc= 0.75812 val_roc= 0.91593 val_ap= 0.93198 test_roc= 0.90728 test_ap= 0.92515\n",
            "Epoch: 0461 train_loss= 0.43144 train_acc= 0.76001 val_roc= 0.91628 val_ap= 0.93206 test_roc= 0.90794 test_ap= 0.92546\n",
            "Epoch: 0471 train_loss= 0.43141 train_acc= 0.76117 val_roc= 0.91707 val_ap= 0.93255 test_roc= 0.90831 test_ap= 0.92582\n",
            "Epoch: 0481 train_loss= 0.43112 train_acc= 0.76201 val_roc= 0.91839 val_ap= 0.93328 test_roc= 0.90910 test_ap= 0.92646\n",
            "Epoch: 0491 train_loss= 0.43059 train_acc= 0.76405 val_roc= 0.91933 val_ap= 0.93421 test_roc= 0.90931 test_ap= 0.92669\n",
            "Best epoch (from validation):  496\n",
            "test_roc  0.90942249459005\n",
            "test_ap  0.9268454442686309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQB5BbKOQpQA",
        "outputId": "84a117e2-f383-4d32-a13c-46f4f41a0a59"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lambda=0.8\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.8\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.76185 train_acc= 0.99648 val_roc= 0.60083 val_ap= 0.61264 test_roc= 0.63431 test_ap= 0.64188\n",
            "Epoch: 0011 train_loss= 0.75249 train_acc= 0.94813 val_roc= 0.67210 val_ap= 0.70430 test_roc= 0.71059 test_ap= 0.75116\n",
            "Epoch: 0021 train_loss= 0.74481 train_acc= 0.89634 val_roc= 0.68366 val_ap= 0.71398 test_roc= 0.71620 test_ap= 0.74947\n",
            "Epoch: 0031 train_loss= 0.71990 train_acc= 0.83926 val_roc= 0.68905 val_ap= 0.71501 test_roc= 0.71978 test_ap= 0.74894\n",
            "Epoch: 0041 train_loss= 0.66929 train_acc= 0.75896 val_roc= 0.69878 val_ap= 0.72404 test_roc= 0.70205 test_ap= 0.74122\n",
            "Epoch: 0051 train_loss= 0.63799 train_acc= 0.71817 val_roc= 0.67725 val_ap= 0.68820 test_roc= 0.66929 test_ap= 0.68420\n",
            "Epoch: 0061 train_loss= 0.62172 train_acc= 0.69763 val_roc= 0.71101 val_ap= 0.72352 test_roc= 0.70324 test_ap= 0.72127\n",
            "Epoch: 0071 train_loss= 0.61576 train_acc= 0.67571 val_roc= 0.70764 val_ap= 0.72282 test_roc= 0.71483 test_ap= 0.73540\n",
            "Epoch: 0081 train_loss= 0.61504 train_acc= 0.65402 val_roc= 0.69767 val_ap= 0.68572 test_roc= 0.69266 test_ap= 0.68130\n",
            "Epoch: 0091 train_loss= 0.61098 train_acc= 0.66399 val_roc= 0.71039 val_ap= 0.69561 test_roc= 0.70018 test_ap= 0.68688\n",
            "Epoch: 0101 train_loss= 0.60900 train_acc= 0.65465 val_roc= 0.70925 val_ap= 0.69244 test_roc= 0.70446 test_ap= 0.68895\n",
            "Epoch: 0111 train_loss= 0.60657 train_acc= 0.65878 val_roc= 0.70952 val_ap= 0.69214 test_roc= 0.70763 test_ap= 0.69433\n",
            "Epoch: 0121 train_loss= 0.60485 train_acc= 0.65491 val_roc= 0.70741 val_ap= 0.68224 test_roc= 0.70831 test_ap= 0.68914\n",
            "Epoch: 0131 train_loss= 0.60125 train_acc= 0.65858 val_roc= 0.71305 val_ap= 0.69376 test_roc= 0.71770 test_ap= 0.70151\n",
            "Epoch: 0141 train_loss= 0.60011 train_acc= 0.65593 val_roc= 0.71260 val_ap= 0.68692 test_roc= 0.72095 test_ap= 0.69729\n",
            "Epoch: 0151 train_loss= 0.59389 train_acc= 0.65105 val_roc= 0.71567 val_ap= 0.68225 test_roc= 0.72121 test_ap= 0.68816\n",
            "Epoch: 0161 train_loss= 0.58706 train_acc= 0.64952 val_roc= 0.72615 val_ap= 0.68684 test_roc= 0.72711 test_ap= 0.68467\n",
            "Epoch: 0171 train_loss= 0.57035 train_acc= 0.64567 val_roc= 0.74087 val_ap= 0.69916 test_roc= 0.73875 test_ap= 0.69237\n",
            "Epoch: 0181 train_loss= 0.55069 train_acc= 0.66147 val_roc= 0.74789 val_ap= 0.72248 test_roc= 0.76731 test_ap= 0.74708\n",
            "Epoch: 0191 train_loss= 0.53808 train_acc= 0.65782 val_roc= 0.75827 val_ap= 0.74982 test_roc= 0.79162 test_ap= 0.78569\n",
            "Epoch: 0201 train_loss= 0.53249 train_acc= 0.65654 val_roc= 0.77478 val_ap= 0.77090 test_roc= 0.81302 test_ap= 0.82063\n",
            "Epoch: 0211 train_loss= 0.52769 train_acc= 0.65467 val_roc= 0.78476 val_ap= 0.78651 test_roc= 0.83358 test_ap= 0.84820\n",
            "Epoch: 0221 train_loss= 0.52218 train_acc= 0.65940 val_roc= 0.79119 val_ap= 0.79160 test_roc= 0.83553 test_ap= 0.85005\n",
            "Epoch: 0231 train_loss= 0.52097 train_acc= 0.65577 val_roc= 0.80475 val_ap= 0.80680 test_roc= 0.84710 test_ap= 0.86432\n",
            "Epoch: 0241 train_loss= 0.51112 train_acc= 0.66824 val_roc= 0.81035 val_ap= 0.81168 test_roc= 0.84953 test_ap= 0.86765\n",
            "Epoch: 0251 train_loss= 0.50648 train_acc= 0.67101 val_roc= 0.82249 val_ap= 0.82252 test_roc= 0.85617 test_ap= 0.87407\n",
            "Epoch: 0261 train_loss= 0.50240 train_acc= 0.67732 val_roc= 0.83403 val_ap= 0.83369 test_roc= 0.86275 test_ap= 0.88117\n",
            "Epoch: 0271 train_loss= 0.49761 train_acc= 0.67835 val_roc= 0.83819 val_ap= 0.83960 test_roc= 0.86741 test_ap= 0.88638\n",
            "Epoch: 0281 train_loss= 0.49304 train_acc= 0.68695 val_roc= 0.84815 val_ap= 0.84906 test_roc= 0.87390 test_ap= 0.89186\n",
            "Epoch: 0291 train_loss= 0.48978 train_acc= 0.68963 val_roc= 0.85651 val_ap= 0.85863 test_roc= 0.88057 test_ap= 0.89767\n",
            "Epoch: 0301 train_loss= 0.48792 train_acc= 0.68913 val_roc= 0.86529 val_ap= 0.86544 test_roc= 0.88712 test_ap= 0.90318\n",
            "Epoch: 0311 train_loss= 0.48604 train_acc= 0.69336 val_roc= 0.86472 val_ap= 0.86523 test_roc= 0.88797 test_ap= 0.90490\n",
            "Epoch: 0321 train_loss= 0.48465 train_acc= 0.69386 val_roc= 0.87467 val_ap= 0.87128 test_roc= 0.89359 test_ap= 0.90957\n",
            "Epoch: 0331 train_loss= 0.48376 train_acc= 0.69378 val_roc= 0.87304 val_ap= 0.87222 test_roc= 0.89423 test_ap= 0.91124\n",
            "Epoch: 0341 train_loss= 0.48288 train_acc= 0.69506 val_roc= 0.88064 val_ap= 0.87768 test_roc= 0.89879 test_ap= 0.91435\n",
            "Epoch: 0351 train_loss= 0.48117 train_acc= 0.69540 val_roc= 0.88373 val_ap= 0.88006 test_roc= 0.90056 test_ap= 0.91546\n",
            "Epoch: 0361 train_loss= 0.48067 train_acc= 0.69775 val_roc= 0.88631 val_ap= 0.88418 test_roc= 0.90247 test_ap= 0.91742\n",
            "Epoch: 0371 train_loss= 0.47943 train_acc= 0.69956 val_roc= 0.88865 val_ap= 0.88579 test_roc= 0.90380 test_ap= 0.91831\n",
            "Epoch: 0381 train_loss= 0.47756 train_acc= 0.70062 val_roc= 0.89095 val_ap= 0.88907 test_roc= 0.90540 test_ap= 0.91962\n",
            "Epoch: 0391 train_loss= 0.47581 train_acc= 0.70287 val_roc= 0.89401 val_ap= 0.89336 test_roc= 0.90708 test_ap= 0.92092\n",
            "Epoch: 0401 train_loss= 0.47426 train_acc= 0.70594 val_roc= 0.90049 val_ap= 0.90087 test_roc= 0.91005 test_ap= 0.92292\n",
            "Epoch: 0411 train_loss= 0.47110 train_acc= 0.71276 val_roc= 0.90351 val_ap= 0.90351 test_roc= 0.91184 test_ap= 0.92440\n",
            "Epoch: 0421 train_loss= 0.46834 train_acc= 0.71701 val_roc= 0.90610 val_ap= 0.90732 test_roc= 0.91186 test_ap= 0.92488\n",
            "Epoch: 0431 train_loss= 0.46496 train_acc= 0.72236 val_roc= 0.90931 val_ap= 0.91087 test_roc= 0.91165 test_ap= 0.92453\n",
            "Epoch: 0441 train_loss= 0.46268 train_acc= 0.72772 val_roc= 0.91239 val_ap= 0.91450 test_roc= 0.91230 test_ap= 0.92526\n",
            "Epoch: 0451 train_loss= 0.46033 train_acc= 0.73088 val_roc= 0.91334 val_ap= 0.91519 test_roc= 0.91384 test_ap= 0.92694\n",
            "Epoch: 0461 train_loss= 0.45976 train_acc= 0.73219 val_roc= 0.91464 val_ap= 0.91648 test_roc= 0.91424 test_ap= 0.92782\n",
            "Epoch: 0471 train_loss= 0.45847 train_acc= 0.73287 val_roc= 0.91579 val_ap= 0.91733 test_roc= 0.91563 test_ap= 0.92949\n",
            "Epoch: 0481 train_loss= 0.45747 train_acc= 0.73496 val_roc= 0.91590 val_ap= 0.91745 test_roc= 0.91572 test_ap= 0.93008\n",
            "Epoch: 0491 train_loss= 0.45626 train_acc= 0.73628 val_roc= 0.91667 val_ap= 0.91864 test_roc= 0.91682 test_ap= 0.93092\n",
            "Best epoch (from validation):  489\n",
            "test_roc  0.9171278476500473\n",
            "test_ap  0.9309560994900189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j3a8CAQpYf",
        "outputId": "c7f59b99-ad62-48a4-b1ba-dbaf4dd767db"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lamdba = 0.9\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 0.9\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.78699 train_acc= 0.99841 val_roc= 0.63502 val_ap= 0.66323 test_roc= 0.67740 test_ap= 0.71324\n",
            "Epoch: 0011 train_loss= 0.77802 train_acc= 0.99758 val_roc= 0.68757 val_ap= 0.72600 test_roc= 0.72595 test_ap= 0.76507\n",
            "Epoch: 0021 train_loss= 0.77110 train_acc= 0.98857 val_roc= 0.70644 val_ap= 0.73314 test_roc= 0.73396 test_ap= 0.76822\n",
            "Epoch: 0031 train_loss= 0.76405 train_acc= 0.87783 val_roc= 0.71124 val_ap= 0.73672 test_roc= 0.73476 test_ap= 0.76635\n",
            "Epoch: 0041 train_loss= 0.76527 train_acc= 0.88387 val_roc= 0.71116 val_ap= 0.73696 test_roc= 0.73463 test_ap= 0.76287\n",
            "Epoch: 0051 train_loss= 0.75288 train_acc= 0.83304 val_roc= 0.70663 val_ap= 0.73021 test_roc= 0.72616 test_ap= 0.75189\n",
            "Epoch: 0061 train_loss= 0.74120 train_acc= 0.91046 val_roc= 0.71467 val_ap= 0.73030 test_roc= 0.73453 test_ap= 0.75549\n",
            "Epoch: 0071 train_loss= 0.71377 train_acc= 0.83970 val_roc= 0.71606 val_ap= 0.72766 test_roc= 0.71483 test_ap= 0.73620\n",
            "Epoch: 0081 train_loss= 0.68754 train_acc= 0.77926 val_roc= 0.59180 val_ap= 0.59500 test_roc= 0.59383 test_ap= 0.57540\n",
            "Epoch: 0091 train_loss= 0.66417 train_acc= 0.76242 val_roc= 0.68315 val_ap= 0.62988 test_roc= 0.66625 test_ap= 0.60496\n",
            "Epoch: 0101 train_loss= 0.65233 train_acc= 0.69827 val_roc= 0.66933 val_ap= 0.62729 test_roc= 0.65384 test_ap= 0.60234\n",
            "Epoch: 0111 train_loss= 0.64311 train_acc= 0.68922 val_roc= 0.67822 val_ap= 0.63525 test_roc= 0.66928 test_ap= 0.61839\n",
            "Epoch: 0121 train_loss= 0.63613 train_acc= 0.66303 val_roc= 0.67908 val_ap= 0.64252 test_roc= 0.67806 test_ap= 0.62601\n",
            "Epoch: 0131 train_loss= 0.63471 train_acc= 0.64732 val_roc= 0.68143 val_ap= 0.64042 test_roc= 0.68520 test_ap= 0.62795\n",
            "Epoch: 0141 train_loss= 0.63191 train_acc= 0.65768 val_roc= 0.68550 val_ap= 0.65963 test_roc= 0.68908 test_ap= 0.63963\n",
            "Epoch: 0151 train_loss= 0.63008 train_acc= 0.64127 val_roc= 0.69481 val_ap= 0.67006 test_roc= 0.69482 test_ap= 0.64629\n",
            "Epoch: 0161 train_loss= 0.63049 train_acc= 0.65255 val_roc= 0.70264 val_ap= 0.67544 test_roc= 0.69627 test_ap= 0.64943\n",
            "Epoch: 0171 train_loss= 0.62736 train_acc= 0.64364 val_roc= 0.70542 val_ap= 0.67695 test_roc= 0.69538 test_ap= 0.64883\n",
            "Epoch: 0181 train_loss= 0.62806 train_acc= 0.64002 val_roc= 0.70654 val_ap= 0.67702 test_roc= 0.69562 test_ap= 0.65040\n",
            "Epoch: 0191 train_loss= 0.62533 train_acc= 0.63993 val_roc= 0.70764 val_ap= 0.67770 test_roc= 0.69425 test_ap= 0.64839\n",
            "Epoch: 0201 train_loss= 0.62520 train_acc= 0.63719 val_roc= 0.71129 val_ap= 0.68181 test_roc= 0.69388 test_ap= 0.64800\n",
            "Epoch: 0211 train_loss= 0.62589 train_acc= 0.63833 val_roc= 0.71100 val_ap= 0.68067 test_roc= 0.69271 test_ap= 0.64661\n",
            "Epoch: 0221 train_loss= 0.62342 train_acc= 0.63083 val_roc= 0.70887 val_ap= 0.67767 test_roc= 0.69434 test_ap= 0.64769\n",
            "Epoch: 0231 train_loss= 0.62241 train_acc= 0.64077 val_roc= 0.71250 val_ap= 0.68147 test_roc= 0.69408 test_ap= 0.64775\n",
            "Epoch: 0241 train_loss= 0.62214 train_acc= 0.63630 val_roc= 0.71921 val_ap= 0.69526 test_roc= 0.70204 test_ap= 0.65916\n",
            "Epoch: 0251 train_loss= 0.62009 train_acc= 0.64462 val_roc= 0.71995 val_ap= 0.69921 test_roc= 0.70200 test_ap= 0.65916\n",
            "Epoch: 0261 train_loss= 0.61993 train_acc= 0.63956 val_roc= 0.72173 val_ap= 0.70147 test_roc= 0.70342 test_ap= 0.66016\n",
            "Epoch: 0271 train_loss= 0.61918 train_acc= 0.63076 val_roc= 0.72368 val_ap= 0.70254 test_roc= 0.70443 test_ap= 0.65937\n",
            "Epoch: 0281 train_loss= 0.62003 train_acc= 0.63795 val_roc= 0.72427 val_ap= 0.70301 test_roc= 0.70449 test_ap= 0.65919\n",
            "Epoch: 0291 train_loss= 0.61829 train_acc= 0.64290 val_roc= 0.72245 val_ap= 0.70099 test_roc= 0.70345 test_ap= 0.65794\n",
            "Epoch: 0301 train_loss= 0.61842 train_acc= 0.63508 val_roc= 0.72279 val_ap= 0.70190 test_roc= 0.70406 test_ap= 0.65804\n",
            "Epoch: 0311 train_loss= 0.61845 train_acc= 0.63781 val_roc= 0.72339 val_ap= 0.70244 test_roc= 0.70396 test_ap= 0.65821\n",
            "Epoch: 0321 train_loss= 0.61839 train_acc= 0.64069 val_roc= 0.72184 val_ap= 0.69869 test_roc= 0.70351 test_ap= 0.65637\n",
            "Epoch: 0331 train_loss= 0.61927 train_acc= 0.63871 val_roc= 0.72555 val_ap= 0.70403 test_roc= 0.70503 test_ap= 0.66067\n",
            "Epoch: 0341 train_loss= 0.61988 train_acc= 0.64209 val_roc= 0.72308 val_ap= 0.70125 test_roc= 0.70373 test_ap= 0.65850\n",
            "Epoch: 0351 train_loss= 0.61812 train_acc= 0.64699 val_roc= 0.72306 val_ap= 0.70033 test_roc= 0.70394 test_ap= 0.65841\n",
            "Epoch: 0361 train_loss= 0.61576 train_acc= 0.63650 val_roc= 0.72521 val_ap= 0.70332 test_roc= 0.70470 test_ap= 0.66095\n",
            "Epoch: 0371 train_loss= 0.61685 train_acc= 0.63829 val_roc= 0.72543 val_ap= 0.70392 test_roc= 0.70387 test_ap= 0.65961\n",
            "Epoch: 0381 train_loss= 0.61651 train_acc= 0.64444 val_roc= 0.72258 val_ap= 0.70171 test_roc= 0.70218 test_ap= 0.65822\n",
            "Epoch: 0391 train_loss= 0.61595 train_acc= 0.64080 val_roc= 0.72284 val_ap= 0.70266 test_roc= 0.70210 test_ap= 0.65802\n",
            "Epoch: 0401 train_loss= 0.61625 train_acc= 0.64726 val_roc= 0.71790 val_ap= 0.69503 test_roc= 0.70150 test_ap= 0.65539\n",
            "Epoch: 0411 train_loss= 0.61695 train_acc= 0.64749 val_roc= 0.71969 val_ap= 0.69932 test_roc= 0.70263 test_ap= 0.65906\n",
            "Epoch: 0421 train_loss= 0.61596 train_acc= 0.64679 val_roc= 0.72111 val_ap= 0.70072 test_roc= 0.70327 test_ap= 0.65928\n",
            "Epoch: 0431 train_loss= 0.61604 train_acc= 0.63692 val_roc= 0.72075 val_ap= 0.70003 test_roc= 0.70332 test_ap= 0.65931\n",
            "Epoch: 0441 train_loss= 0.61495 train_acc= 0.63915 val_roc= 0.72039 val_ap= 0.69943 test_roc= 0.70308 test_ap= 0.65946\n",
            "Epoch: 0451 train_loss= 0.61493 train_acc= 0.64473 val_roc= 0.72112 val_ap= 0.70103 test_roc= 0.70352 test_ap= 0.66078\n",
            "Epoch: 0461 train_loss= 0.61389 train_acc= 0.64267 val_roc= 0.72079 val_ap= 0.69961 test_roc= 0.70288 test_ap= 0.65984\n",
            "Epoch: 0471 train_loss= 0.61269 train_acc= 0.64634 val_roc= 0.72095 val_ap= 0.69946 test_roc= 0.70195 test_ap= 0.65993\n",
            "Epoch: 0481 train_loss= 0.61161 train_acc= 0.65353 val_roc= 0.71924 val_ap= 0.69791 test_roc= 0.70184 test_ap= 0.65990\n",
            "Epoch: 0491 train_loss= 0.60788 train_acc= 0.65249 val_roc= 0.71824 val_ap= 0.69798 test_roc= 0.70380 test_ap= 0.66234\n",
            "Best epoch (from validation):  67\n",
            "test_roc  0.7254049811146838\n",
            "test_ap  0.7482264469999393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUupUXE7QphC",
        "outputId": "568879f7-4c2f-445a-a6fe-34c6c4370de2"
      },
      "source": [
        "#Now we want to check how autoregressive-scalar changes our roc/ap lambda = 1\r\n",
        "!bash python train.py --epochs 500 --model feedback --edge_dropout 0.5 --learning_rate 0.01 --autoregressive_scalar 1\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/model.py:157: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "Epoch: 0001 train_loss= 0.80229 train_acc= 0.99841 val_roc= 0.62736 val_ap= 0.64227 test_roc= 0.66156 test_ap= 0.69273\n",
            "Epoch: 0011 train_loss= 0.79219 train_acc= 0.99841 val_roc= 0.64032 val_ap= 0.67239 test_roc= 0.63745 test_ap= 0.66338\n",
            "Epoch: 0021 train_loss= 0.78417 train_acc= 0.99841 val_roc= 0.63090 val_ap= 0.65285 test_roc= 0.64381 test_ap= 0.65973\n",
            "Epoch: 0031 train_loss= 0.78345 train_acc= 0.99841 val_roc= 0.63902 val_ap= 0.66368 test_roc= 0.65629 test_ap= 0.66999\n",
            "Epoch: 0041 train_loss= 0.78395 train_acc= 0.99841 val_roc= 0.63593 val_ap= 0.66507 test_roc= 0.65827 test_ap= 0.68034\n",
            "Epoch: 0051 train_loss= 0.79208 train_acc= 0.99841 val_roc= 0.63266 val_ap= 0.65908 test_roc= 0.65916 test_ap= 0.68059\n",
            "Epoch: 0061 train_loss= 0.78906 train_acc= 0.99841 val_roc= 0.63614 val_ap= 0.65916 test_roc= 0.65781 test_ap= 0.67829\n",
            "Epoch: 0071 train_loss= 0.79097 train_acc= 0.99841 val_roc= 0.63714 val_ap= 0.65797 test_roc= 0.66140 test_ap= 0.67866\n",
            "Epoch: 0081 train_loss= 0.79107 train_acc= 0.99841 val_roc= 0.63719 val_ap= 0.65540 test_roc= 0.66326 test_ap= 0.67870\n",
            "Epoch: 0091 train_loss= 0.79492 train_acc= 0.99841 val_roc= 0.63847 val_ap= 0.65603 test_roc= 0.66282 test_ap= 0.67868\n",
            "Epoch: 0101 train_loss= 0.79460 train_acc= 0.99841 val_roc= 0.63459 val_ap= 0.64886 test_roc= 0.65765 test_ap= 0.67770\n",
            "Epoch: 0111 train_loss= 0.79102 train_acc= 0.99841 val_roc= 0.63563 val_ap= 0.65317 test_roc= 0.65188 test_ap= 0.67194\n",
            "Epoch: 0121 train_loss= 0.78969 train_acc= 0.99841 val_roc= 0.63320 val_ap= 0.64386 test_roc= 0.64553 test_ap= 0.66399\n",
            "Epoch: 0131 train_loss= 0.79077 train_acc= 0.99841 val_roc= 0.63711 val_ap= 0.64676 test_roc= 0.64687 test_ap= 0.66430\n",
            "Epoch: 0141 train_loss= 0.78875 train_acc= 0.99841 val_roc= 0.64208 val_ap= 0.65380 test_roc= 0.64881 test_ap= 0.66493\n",
            "Epoch: 0151 train_loss= 0.79002 train_acc= 0.99841 val_roc= 0.64120 val_ap= 0.64905 test_roc= 0.64793 test_ap= 0.66466\n",
            "Epoch: 0161 train_loss= 0.79458 train_acc= 0.99841 val_roc= 0.64551 val_ap= 0.65282 test_roc= 0.65245 test_ap= 0.66885\n",
            "Epoch: 0171 train_loss= 0.79034 train_acc= 0.99841 val_roc= 0.64908 val_ap= 0.65954 test_roc= 0.65839 test_ap= 0.67479\n",
            "Epoch: 0181 train_loss= 0.79164 train_acc= 0.99841 val_roc= 0.64999 val_ap= 0.66046 test_roc= 0.65504 test_ap= 0.67139\n",
            "Epoch: 0191 train_loss= 0.78983 train_acc= 0.99841 val_roc= 0.65088 val_ap= 0.66125 test_roc= 0.65164 test_ap= 0.66830\n",
            "Epoch: 0201 train_loss= 0.79022 train_acc= 0.99841 val_roc= 0.65004 val_ap= 0.66156 test_roc= 0.65151 test_ap= 0.66769\n",
            "Epoch: 0211 train_loss= 0.79210 train_acc= 0.99841 val_roc= 0.65144 val_ap= 0.66324 test_roc= 0.65280 test_ap= 0.66987\n",
            "Epoch: 0221 train_loss= 0.78843 train_acc= 0.99841 val_roc= 0.65061 val_ap= 0.66314 test_roc= 0.65272 test_ap= 0.67156\n",
            "Epoch: 0231 train_loss= 0.79081 train_acc= 0.99841 val_roc= 0.64893 val_ap= 0.66325 test_roc= 0.65586 test_ap= 0.67422\n",
            "Epoch: 0241 train_loss= 0.79022 train_acc= 0.99841 val_roc= 0.64963 val_ap= 0.66668 test_roc= 0.65660 test_ap= 0.67426\n",
            "Epoch: 0251 train_loss= 0.79042 train_acc= 0.99841 val_roc= 0.65073 val_ap= 0.67060 test_roc= 0.65825 test_ap= 0.67404\n",
            "Epoch: 0261 train_loss= 0.78857 train_acc= 0.99841 val_roc= 0.65084 val_ap= 0.67098 test_roc= 0.65723 test_ap= 0.67114\n",
            "Epoch: 0271 train_loss= 0.79094 train_acc= 0.99841 val_roc= 0.64650 val_ap= 0.66397 test_roc= 0.65712 test_ap= 0.66795\n",
            "Epoch: 0281 train_loss= 0.79011 train_acc= 0.99841 val_roc= 0.64900 val_ap= 0.66374 test_roc= 0.65758 test_ap= 0.66751\n",
            "Epoch: 0291 train_loss= 0.78773 train_acc= 0.99841 val_roc= 0.64665 val_ap= 0.65778 test_roc= 0.65617 test_ap= 0.66716\n",
            "Epoch: 0301 train_loss= 0.78811 train_acc= 0.99841 val_roc= 0.65070 val_ap= 0.66267 test_roc= 0.65691 test_ap= 0.66398\n",
            "Epoch: 0311 train_loss= 0.79094 train_acc= 0.99841 val_roc= 0.66116 val_ap= 0.66895 test_roc= 0.66517 test_ap= 0.66007\n",
            "Epoch: 0321 train_loss= 0.79188 train_acc= 0.99841 val_roc= 0.66463 val_ap= 0.67437 test_roc= 0.66704 test_ap= 0.65899\n",
            "Epoch: 0331 train_loss= 0.79176 train_acc= 0.99841 val_roc= 0.67044 val_ap= 0.67553 test_roc= 0.67052 test_ap= 0.65967\n",
            "Epoch: 0341 train_loss= 0.79302 train_acc= 0.99841 val_roc= 0.67590 val_ap= 0.68002 test_roc= 0.67342 test_ap= 0.66202\n",
            "Epoch: 0351 train_loss= 0.79400 train_acc= 0.99841 val_roc= 0.68036 val_ap= 0.67705 test_roc= 0.67307 test_ap= 0.65382\n",
            "Epoch: 0361 train_loss= 0.78954 train_acc= 0.99841 val_roc= 0.67683 val_ap= 0.67390 test_roc= 0.67701 test_ap= 0.65976\n",
            "Epoch: 0371 train_loss= 0.79373 train_acc= 0.99841 val_roc= 0.66693 val_ap= 0.65344 test_roc= 0.67286 test_ap= 0.65666\n",
            "Epoch: 0381 train_loss= 0.79408 train_acc= 0.99841 val_roc= 0.65975 val_ap= 0.66019 test_roc= 0.67971 test_ap= 0.66026\n",
            "Epoch: 0391 train_loss= 0.79164 train_acc= 0.99841 val_roc= 0.65236 val_ap= 0.65489 test_roc= 0.68051 test_ap= 0.66402\n",
            "Epoch: 0401 train_loss= 0.78535 train_acc= 0.99841 val_roc= 0.65149 val_ap= 0.64629 test_roc= 0.68037 test_ap= 0.67008\n",
            "Epoch: 0411 train_loss= 0.78415 train_acc= 0.99841 val_roc= 0.65631 val_ap= 0.65238 test_roc= 0.68391 test_ap= 0.66851\n",
            "Epoch: 0421 train_loss= 0.78471 train_acc= 0.99841 val_roc= 0.65897 val_ap= 0.66115 test_roc= 0.68121 test_ap= 0.67038\n",
            "Epoch: 0431 train_loss= 0.78671 train_acc= 0.99841 val_roc= 0.66481 val_ap= 0.66973 test_roc= 0.68435 test_ap= 0.67741\n",
            "Epoch: 0441 train_loss= 0.78640 train_acc= 0.99841 val_roc= 0.66022 val_ap= 0.68352 test_roc= 0.68381 test_ap= 0.69107\n",
            "Epoch: 0451 train_loss= 0.77821 train_acc= 0.99841 val_roc= 0.67252 val_ap= 0.66311 test_roc= 0.69099 test_ap= 0.68586\n",
            "Epoch: 0461 train_loss= 0.75284 train_acc= 0.95050 val_roc= 0.64366 val_ap= 0.59947 test_roc= 0.63376 test_ap= 0.58730\n",
            "Epoch: 0471 train_loss= 0.71951 train_acc= 0.75658 val_roc= 0.64937 val_ap= 0.61006 test_roc= 0.62910 test_ap= 0.58186\n",
            "Epoch: 0481 train_loss= 0.68907 train_acc= 0.74547 val_roc= 0.69430 val_ap= 0.65327 test_roc= 0.69889 test_ap= 0.68782\n",
            "Epoch: 0491 train_loss= 0.67180 train_acc= 0.74361 val_roc= 0.70016 val_ap= 0.65485 test_roc= 0.69686 test_ap= 0.68317\n",
            "Best epoch (from validation):  343\n",
            "test_roc  0.6726917246668516\n",
            "test_ap  0.6572451800660116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP8PqQ1Od47F"
      },
      "source": [
        "## Results from testing lambda:\r\n",
        "        update = (1 - FLAGS.autoregressive_scalar) * z + FLAGS.autoregressive_scalar * update\r\n",
        "\r\n",
        "So lambda = 0 means VGAE/GAE, and lambda = 1 means use iterative update only, without initial Z\r\n",
        "\r\n",
        "| lambda | roc | ap |\r\n",
        "| --- | --- | --- |\r\n",
        "| 1.0 | .67 | .65 |\r\n",
        "| 0.9 | .72 | .74 |\r\n",
        "| 0.8 | .91 | .93 |\r\n",
        "| 0.7 | .90 | .92 |\r\n",
        "| 0.6 | .90 | .92 |\r\n",
        "| 0.5 | .91 | .93 |\r\n",
        "| 0.4 | .91 | .93 |\r\n",
        "| 0.3 | .92 | .93 |\r\n",
        "| 0.2 | .91 | .93 |\r\n",
        "| 0.1 | .91 | .93 |\r\n",
        "| 0.0 | .92 | .93 |\r\n"
      ]
    }
  ]
}